paper_id,title,abstract,authors,authorids,keywords,primary_area,venue,year,pdf_url,forum_url,author_ids,submission_date,bias_result
ScwthqGzyt,LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization,"Retrieval-augmented generation (RAG) has become a popular approach to improving large language models (LLMs), yet trustworthiness remains a central challenge: models may produce fluent but incorrect answers, and retrieved context can amplify errors when irrelevant or misleading. To address this, we study how model internals reflect the interplay between parametric knowledge and external context during generation. Specifically, we ask: (1) can the correctness of a model’s output be inferred directly from its internal activations, and (2) do these internals reveal whether external context is helpful, harmful, or irrelevant? We introduce metrics grounded in intermediate activations to capture both dimensions. Across six models, a simple classifier trained on hidden states of the first output token predicts output correctness with nearly 75% accuracy, enabling early auditing. Moreover, our internals-based metric substantially outperforms prompting baselines at distinguishing between correct and incorrect context, guarding against polluted retrieval. These findings highlight model activations as a promising lens for understanding and improving the reliability of RAG systems.",,,"Calibration, Uncertainty, Knowledge Tracing, Probing",interpretability and explainable AI,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=ScwthqGzyt,https://openreview.net/forum?id=ScwthqGzyt,,,
nGLefuWScZ,FITS: Conditional Diffusion Model for Irregular Time Series Forecasting with Pseudo-future Exogenous Covariates,"Irregular multivariate time series (IMTS) present unique challenges due to non-uniform intervals and different sampling rates. While existing methods struggle to capture both long-term dynamics and cross-channel dependencies under such irregularities, we tackle this by formulating time series forecasting as a conditional generation problem and introducing FITS, a conditional diffusion model for IMTS forecasting that leverages pseudo-future exogenous covariates. Our approach incorporates two key innovations. First, we propose a novel density-aware adaptive patching scheme that generates data-driven segments with dynamic boundaries determined by the information density. This scheme overcomes the limitations of traditional fixed-length or fixed-span segmentation in preserving continuous local semantics and modeling inter-time series correlations. Second, we develop a transformer-based prior knowledge extractor that captures forward-looking covariate dependencies via a novel cross-variate attention mechanism. The transformer structure is integrated into the conditional diffusion generative process as a unified framework, enabling precise distributional forecasting for IMTS. Extensive experiments on six datasets with four evaluation metrics validate the effectiveness of FITS.",,,time series forecasting,learning on time series and dynamical systems,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=nGLefuWScZ,https://openreview.net/forum?id=nGLefuWScZ,,,
m6W5SfQXrT,High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Scene Representations,"Generalizable neural surface reconstruction has become a compelling technique at reconstructing 3D scenes from sparse input views without per-scene optimization. In these methods, dense 3D feature volumes has proven very effective as a global scene representation. Unfortunately, this representation severely limits their high-resolution modeling abilities and reconstruction accuracies because memory requirements scale cubically with voxel resolution. In this paper, we propose a novel sparse-representation approach that dramatically improves memory efficiency and allows for more accurate surface reconstructions. Our method employs a two-stage pipeline: We first train a neural network to predict voxel occupancy probabilities from the given posed images, then we restrict feature computation and volume rendering to the sparse voxels with sufficiently high occupancy estimates. To support this sparse representation, we develop specialized algorithms for efficient sampling, feature aggregation, and spatial querying that overcome the dense-volume assumptions of existing approaches. Extensive experiments on standard benchmarks demonstrate that our sparse representation enables scene reconstruction at a $512^3$ resolution, compared to the typical $128^3$ resolution possible with existing methods on similar hardware. We also achieve superior reconstruction accuracy compared to current state-of-the-art approaches. Our work establishes sparse neural representations as a promising direction for scalable, high-quality 3D reconstruction.",,,"neural surface reconstruction, neural radiance fields, multi-view stereo, 3D reconstruction","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=m6W5SfQXrT,https://openreview.net/forum?id=m6W5SfQXrT,,,
KTyMKIHbg3,A UCB-type of Approach for Nonstationary MDPs with General Function Approximation,"Function approximation has experienced significant success in the field of reinforcement learning (RL). Despite a handful of progress on developing theory for Nonstationary RL with function approximation under structural assumptions, existing work for nonstationary RL with general function approximation \citet{Feng:nonstationary:ICML:2023} studied the confidence-set based algorithm relying on an oracle to select the optimistic state-action value function within the confidence set, which is computationally inefficient. To mitigate the drawback of confidence-set based algorithm, in this work, we propose a popular UCB-type of algorithm for nonstationary RL with general function approximation. Our algorithm features the restart mechanism and a new design of bonus term to handle nonstationarity. We then establish a dynamic regret upper bound for the proposed algorithm, and demonstrate the dynamic regret bound for the examples of nonstationary tabular MDPs and nonstationary linear MDPs. To the best of our knowledge, this is the first UCB-type of algorithm for non-stationary RL with general function approximation. Our theory contributes to the recent progress on RL with general function approximation.","Songtao Feng, Ming Yin, Jie Fu, Jing Yang, Yingbin Liang","~Songtao_Feng1, ~Ming_Yin4, ~Jie_Fu5, ~Jing_Yang3, ~Yingbin_Liang1","nonstationary MDP, general function approximation, eluder dimension",reinforcement learning,ICLR 2024 Conference Withdrawn Submission,2024,https://openreview.net/pdf?id=KTyMKIHbg3,https://openreview.net/forum?id=KTyMKIHbg3,,,
Pa7oHHhqFa,Language Models as Noisy Experts for Sequential Causal Discovery,"Causal discovery from observational data typically assumes access to complete data and availability of domain experts. In practice, data often arrive in batches, are subject to sampling bias, and expert knowledge is scarce. Language Models (LMs) offer a surrogate for expert knowledge but suffer from hallucinations, inconsistencies, and bias. We present a hybrid framework that bridges these gaps by adaptively integrating sequential batch data with LM-derived noisy, expert knowledge while accounting for both *data-induced* and *LM-induced* biases. We propose a representation shift from Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG), that accommodates ambiguities within a coherent framework, allowing grounding the *global* LM knowledge in *local* observational data. To guide LM interactions, we use a sequential optimization scheme that adaptively queries the most informative edges. Across varied datasets, we outperform prior work in structural accuracy and extend to parameter estimation, showing robustness to LM noise.",,,"Causal Discovery, Sequential Data, Noisy Expert, Language Model","probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=Pa7oHHhqFa,https://openreview.net/forum?id=Pa7oHHhqFa,,,
QcA9iGaLpH4,What do large networks memorize?,"The success of modern neural models has prompted renewed study of the connection between memorisation and generalisation: such models typically generalise well, despite being able to perfectly fit (""memorise"") completely random labels.
To more carefully study this issue, Feldman (2019); Feldman & Zhang (2020) provided a simple metric to quantify the degree of memorisation of a specific training example, and empirically quantified the corresponding memorisation profile of a ResNet model on image classification benchmarks.
While an exciting first glimpse into how real-world models memorise, these studies leave open several questions about memorisation of practical networks.
In particular, how is memorisation affected by increasing model size, and by distilling a large model into a smaller one?
We present a systematic empirical analysis of these questions.
On standard image classification benchmarks, we find that training examples exhibit a diverse set of memorisation trajectories across model sizes, with some samples having increased memorisation under larger models.
Further, we find that distillation tends to inhibit memorisation of the student model, while also improving generalisation.
Finally, we show that computationally tractable measures of memorisation do not capture the properties we identify for memorisation in the sense of Feldman (2019), despite highly correlating to the latter.","Michal Lukasik, Aditya Krishna Menon, Ankit Singh Rawat, Vaishnavh Nagarajan, Sanjiv Kumar",,"memorization, overparameterization, example difficulty",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=QcA9iGaLpH4,https://openreview.net/forum?id=QcA9iGaLpH4,"~Michal_Lukasik1, ~Aditya_Krishna_Menon1, ~Ankit_Singh_Rawat1, ~Vaishnavh_Nagarajan3, ~Sanjiv_Kumar1",1660000000000.0,
pX6B28ynNh,CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers,"Large language models (LLMs) have demonstrated remarkable progress in coding and mathematical problem-solving; however, evaluation on advanced research-level problems in the hard sciences remains scarce. 
To fill this gap, we present \cmt, a dataset of 50 original problems covering condensed matter theory (CMT) at the level of an expert researcher. The topics cover analytical and computational approaches commonly used in quantum many-body physics as well as classical statistical mechanics. This dataset was designed and verified by a panel of expert researchers from around the world. 
We built the dataset through a collaborative environment that challenges the panel to write and refine difficult problems that the panel would like their research assistants to be able to solve, with topics including Hartree-Fock mean-field theory, exact diagonalization, quantum Monte Carlo, density matrix renormalization group, quantum statistical mechanics, classical statistical mechanics, and model building. We evaluate different LLMs by programmatically checking LLM-generated solutions against expert-supplied ground truth. 
For this, we developed machine-grading mechanisms that are suitable for advanced physics research problems. 
For example, we handle non-commuting operators that are essential for quantum many-body problems by symbolic manipulation and normal ordering. 
Our evaluations show that frontier models struggle with all of the problems in the dataset, highlighting a gap in the physical reasoning skills of current LLMs. Notably, experts identified strategies for creating increasingly difficult problems by interacting with the LLMs and exploiting common failure modes. 
While the highest-performing model, GPT5, correctly solves 30\% of the problems, average performance across 17 models (GPT, Gemini, Claude, DeepSeek, and Llama classes) is only 11.4$\pm$2.1\%.  Moreover, our benchmark contains 18 problems that {\it not a single one} of the 17 models can correctly solve, and 26 problems that are solved by {\it at most} one model. 
These currently unsolvable problems span the fields of Quantum Monte Carlo, Variational Monte Carlo, and Density Matrix Renormalization Group. 
The answers sometimes violate fundamental symmetries or have unphysical scaling dimensions. We believe that this benchmark set provides valuable guidance for the future development of language models, aiming to achieve the goal of AI research assistants and tutors.",,,"large language model, statistical mechanics, benchmark, evaluation, numerical methods, scientific problem solving, condensed matter physics, quantum physics",datasets and benchmarks,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=pX6B28ynNh,https://openreview.net/forum?id=pX6B28ynNh,,,
LnKLT5apxB,SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation,"Sign language generation faces the challenge of producing natural and expressive results due to the complexity of sign language, which involves hand gestures, facial expressions, and body movements. In this work, we propose a novel method called SignAligner for realistic sign language generation. The framework consists of three stages: text-driven multimodal co-generation, online collaborative correction, and realistic video synthesis. First, a joint generator incorporating a Transformer-based text encoder and cross-modal attention simultaneously produces posture, gesture, and body movements from text. Next, an online correction module refines the generated modalities using dynamic loss weighting and cross-modal attention to resolve spatiotemporal conflicts and enhance semantic consistency. Finally, the corrected poses are input into a pre-trained video generation network to synthesize high-fidelity sign language videos. Additionally, we introduce a dataset extension scheme that derives three new landmark representations (i.e., Pose, Hamer, and Smplerx) via pre-trained models, validated on PHOENIX14T and CSL-daily. Extensive experiments show that SignAligner significantly improves the accuracy and expressiveness of generated sign videos.",,,"sign language generation, sign language production","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=LnKLT5apxB,https://openreview.net/forum?id=LnKLT5apxB,,,
fDZumshwym,Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation,"Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods.
To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level features. Another helpful property of the hierarchical architecture is that HMN naturally ensures good independence among images despite achieving information sharing. This enables instance-level pruning for HMN to reduce redundant information, thereby further minimizing redundancy and enhancing performance. We evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and Tiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results show that our proposed method outperforms all baselines, even when trained with a batch-based loss consuming less GPU memory.","Haizhong Zheng, Jiachen Sun, Shutong Wu, Bhavya Kailkhura, Zhuoqing Mao, Chaowei Xiao, Atul Prakash","~Haizhong_Zheng1, ~Jiachen_Sun1, ~Shutong_Wu1, ~Bhavya_Kailkhura1, ~Zhuoqing_Mao1, ~Chaowei_Xiao2, ~Atul_Prakash1",Data condensation; Data distillation; Data parameterization,"general machine learning (i.e., none of the above)",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=fDZumshwym,https://openreview.net/forum?id=fDZumshwym,,,
HuNIgYhBoy,Benchmarking World-Model Learning,"Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose _WorldTest_, a protocol for evaluating model-learning agents that separates reward-free interaction from a scored test phase in a different, related
environment. WorldTest is open-ended—models should support many different tasks unknown ahead of time—and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with _AutumnBench_, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template—reward-free exploration, derived tests, and behavior-based scoring—for evaluating what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.",,,"world model learning, benchmark, representation-agnostic evaluation, reward-free learning",datasets and benchmarks,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=HuNIgYhBoy,https://openreview.net/forum?id=HuNIgYhBoy,,,
H7Edu1_IZgR,Transformers are Meta-Reinforcement Learners,"The transformer architecture and variants presented a remarkable success across many machine learning tasks in recent years. This success is intrinsically related to the capability of handling long sequences and the presence of context-dependent weights from the attention mechanism. We argue that these capabilities suit the central role of a Meta-Reinforcement Learning algorithm. Indeed, a meta-RL agent needs to infer the task from a sequence of trajectories. Furthermore, it requires a fast adaptation strategy to adapt its policy for a new task - which can be achieved using the self-attention mechanism. In this work, we present TrMRL (Transformers for Meta-Reinforcement Learning), a meta-RL agent that mimics the memory reinstatement mechanism using the transformer architecture. It associates the recent past of working memories to build an episodic memory recursively through the transformer layers. This memory works as a proxy to the current task, and we condition a policy head on it. We conducted experiments in high-dimensional continuous control environments for locomotion and dexterous manipulation. Results show that TrMRL achieves or surpasses state-of-the-art performance, sample efficiency, and out-of-distribution generalization in these environments.",Luckeciano Carvalho Melo,,"Reinforcement Learning, Meta-Reinforcement Learning, Transformers",,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=H7Edu1_IZgR,https://openreview.net/forum?id=H7Edu1_IZgR,~Luckeciano_Carvalho_Melo1,1630000000000.0,
S1hTxWE7cJ,"X-SHOT: A Single System to Handle Frequent, Few-shot and Zero-shot Labels in Classification","In recent years, few-shot and zero-shot learning, which focus on labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat freq-shot (labels with numerous instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some labels might appear thousands of times, while others might only appear sporadically or not at all. Ideally, a system should accommodate any label, regardless of its training frequency. Notably, while few-shot systems often falter on zero-shot tasks, zero-shot systems don't leverage available annotations when certain downstream labels possess them. For practical deployment, it's crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: $X$-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels emerge without predefined limits. Here, $X$ can span from 0 to positive infinity. The crux of $X$-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. Our solution leverages Instruction Learning, bolstered by data autonomously generated by pre-trained Language Models (PLMs). Our unified system, $X$-shot, surpasses preceding state-of-the-art techniques on three benchmark datasets across diverse domains in both single-label and multi-label classifications. This is the first work addressing $X$-shot learning, where $X$ remains variable.","Hanzi Xu, Slobodan Vucetic, Wenpeng Yin","~Hanzi_Xu1, ~Slobodan_Vucetic1, ~Wenpeng_Yin1","Natural Language Processing, Few-shot Learning, Zero-shot Learning","unsupervised, self-supervised, semi-supervised, and supervised representation learning",ICLR 2024 Conference Withdrawn Submission,2024,https://openreview.net/pdf?id=S1hTxWE7cJ,https://openreview.net/forum?id=S1hTxWE7cJ,,,
CuV_qYkmKb3,Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption,"Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world \emph{tabular} datasets. We propose \textsc{Scarf}, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, \textsc{Scarf} not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that \textsc{Scarf} complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.","Dara Bahri, Heinrich Jiang, Yi Tay, Donald Metzler",,"self-supervised learning, tabular data, pre-training, contrastive learning, openML",,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=CuV_qYkmKb3,https://openreview.net/forum?id=CuV_qYkmKb3,"~Dara_Bahri1, ~Heinrich_Jiang1, ~Yi_Tay1, ~Donald_Metzler1",1630000000000.0,
zfAcPuIHkE,Logic Channel Validation and Enhancement of Zero-Shot Vision-Language Comprehension on Vision Language Models,"Frontier Large Vision-Language Models (LVLMs) exhibit remarkable capabilities in Visual-Language Comprehension (VLC) tasks, enabled by pretraining on vast visual-textual corpus. However, they are often deployed as zero-shot solution in a black-box manner, as retraining challenges remain due to data privacy or model inaccessibility. Validating and understanding the behavior of the models become important for generalization to new task. We propose a Logic Channel, in parallel with the black-box model channel, to perform explicit logic reasoning for validation and enhancement. The frontier LVLM, encapsulating latent vision-language knowledge, can be considered as an Implicit Logic Channel. The proposed Explicit Logic Channel, mimicking human logic reasoning, incorporates a Large Language Model (LLM), a Visual Foundation Model (VFM), and a logical reasoning module involving novel probabilistic inference for factual, counterfactual, relational, and causal condition reasoning over the extracted and grounded visual-textual facts. Cross-channel logic consistency analysis enables model validation and selection, even without ground-truth annotations. Additionally, cross-channel integration further improves performance in zero-shot tasks over SOTA models. Our experiments on three recent challenging VLC benchmarks, Neg- Bench, HC-RefCOCOg, and HC-RefLoCo, demonstrate the effectiveness of the proposed Logic Channel for logic-based model validation, selection and improvement on LVLM with enhanced explainability and trustworthiness.",,,"Vision-Language Model, Visual-Language Comprehension, logic reasoning, zero-shot",interpretability and explainable AI,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=zfAcPuIHkE,https://openreview.net/forum?id=zfAcPuIHkE,,,
s3IBHTTDYl,Language Models Need Inductive Biases to Count Inductively,"Counting constitutes a core skill underlying a wide range of tasks, such as formal language recognition, multi-hop reasoning and simulating algorithms. Generaliz- ing counting inductively is central to task success on out-of-distribution (OOD) instances where testing inputs are longer than those seen in training. While there is a large body of literature reporting poor length generalization in language models, few papers have tried to distill the “reasoning” failure to the simplest case of count- ing failure. We aim to provide a broader picture on whether various language model architectures can a) learn to count, and b) generalize counting inductively. This work provides extensive empirical results on architectures ranging from RNNs, Transformers, State-Space Models and RWKV. We present carefully-designed task formats, auxiliary tasks and positional embeddings to avoid limitations in general- ization with OOD-position and OOD-vocabulary. We find that while traditional RNNs trivially achieve inductive counting, Transformers have to rely on positional embeddings (PEs) to count OOD. Further analyses on interpreting the learned solution reveal that different PEs encode different inductive biases that facilitate counting in different task formats. As counting is the basis for many arguments concerning the expressivity of Transformers, our finding calls for the community to reexamine the application scope of primitive functions defined in formal charac- terizations. Finally, modern RNNs also largely underperform traditional RNNs in generalizing counting inductively, hinting at the tradeoff modern RNNs struggle to balance between parallelized training and maintaining their recurrent nature.","Yingshan Chang, Yonatan Bisk","~Yingshan_Chang1, ~Yonatan_Bisk1","Language Model Architecture, Inductive bias, Expressivity, Counting, Inductive Generalization","foundation or frontier models, including LLMs",ICLR 2025 Poster,2025,https://openreview.net/pdf?id=s3IBHTTDYl,https://openreview.net/forum?id=s3IBHTTDYl,,,
xYGNO86OWDH,Isotropy in the Contextual Embedding Space: Clusters and Manifolds,"The geometric properties of contextual embedding spaces for deep language models such as BERT and ERNIE, have attracted considerable attention in recent years. Investigations on the contextual embeddings demonstrate a strong anisotropic space such that most of the vectors fall within a narrow cone, leading to high cosine similarities.  It is surprising that these LMs are as successful as they are, given that most of their embedding vectors are as similar to one another as they are. In this paper, we argue that the isotropy indeed exists in the space, from a different but more constructive perspective. We identify isolated clusters and low dimensional manifolds in the contextual embedding space, and introduce tools to both qualitatively and quantitatively analyze them. We hope the study in this paper could provide insights towards a better understanding of the deep language models.","Xingyu Cai, Jiaji Huang, Yuchen Bian, Kenneth Church",,"Contextual embedding space, Isotropy, Clusters, Manifolds",,ICLR.cc/2021/Conference,2021,https://openreview.net/pdf?id=xYGNO86OWDH,https://openreview.net/forum?id=xYGNO86OWDH,"~Xingyu_Cai1, ~Jiaji_Huang1, ~Yuchen_Bian1, ~Kenneth_Church1",1600000000000.0,
Ojpb1y8jflw,StyleMorph: Disentangled 3D-Aware Image Synthesis with a 3D Morphable StyleGAN,"We introduce StyleMorph, a 3D-aware generative model that disentangles 3D shape, camera pose, object appearance, and background appearance for high quality image synthesis. We account for shape variability by morphing a canonical 3D object template, effectively learning a 3D morphable model in an entirely unsupervised manner through backprop. We chain 3D morphable modelling with deferred neural rendering by performing an implicit surface rendering of “Template Object Coordinates” (TOCS), which can be understood as an unsupervised counterpart to UV maps. This provides a detailed 2D TOCS map signal that reflects the compounded geometric effects of non-rigid shape variation, camera pose, and perspective projection. We combine 2D TOCS maps with an independent appearance code to condition a StyleGAN-based deferred neural rendering (DNR) network for foreground image (object) synthesis; we use a separate code for background synthesis and do late fusion to deliver the final result. We show competitive synthesis results on 4 datasets (FFHQ faces, AFHQ Cats, Dogs, Wild), while achieving the joint disentanglement of shape, pose, object and background texture.","Eric-Tuan Le, Edward Bartrum, Iasonas Kokkinos",,"3D-aware GAN, Template-based, Morphable, Disentanglement, Photorealistic, Neural Radiance Field, StyleGAN",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=Ojpb1y8jflw,https://openreview.net/forum?id=Ojpb1y8jflw,"~Eric-Tuan_Le1, ~Edward_Bartrum1, ~Iasonas_Kokkinos1",1660000000000.0,
4l5iO9eoh3f,Supervised Permutation Invariant Networks for solving the CVRP with bounded fleet size,"Learning to solve combinatorial optimization problems, such as the vehicle routing problem, offers great computational advantages over classical operation research solvers and heuristics. The recently developed deep reinforcement learning approaches either improve an initially given solution iteratively or sequentially construct a set of individual tours. 
However, all existing learning-based approaches are not able to work for a fixed number of vehicles and thus bypass the NP-hardness of the original problem. On the other hand, this makes them less suitable for real applications, as many logistic service providers rely on solutions provided for a specific bounded fleet size and cannot accommodate short term changes to the number of vehicles.
In contrast we propose a powerful supervised deep learning framework that constructs a complete tour plan from scratch while respecting an apriori fixed number of vehicles. 
In combination with an efficient post-processing scheme, our supervised approach is not only much faster and easier to train but also achieves competitive results that incorporate the practical aspect of vehicle costs.
In thorough controlled experiments we re-evaluate and compare our method to multiple state-of-the-art approaches where we demonstrate stable performance and shed some light on existent inconsistencies in the experimentation protocols of the related work.","Daniela Thyssens, Jonas Falkner, Lars Schmidt-Thieme",,"Deep Learning, Combinatorial Optimization, Vehicle Routing",,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=4l5iO9eoh3f,https://openreview.net/forum?id=4l5iO9eoh3f,"~Daniela_Thyssens1, ~Jonas_Falkner1, ~Lars_Schmidt-Thieme1",1630000000000.0,
ElI9znK_eUz,Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning,"Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like Procgen, and provides a pathway towards more efficient progress.","Lakshmi Narasimhan Govindarajan, Rex G Liu, Drew Linsley, Alekh Karkada Ashok, Max Reuter, Michael Frank, Thomas Serre",,"Cognitive Science, Deep Reinforcement Learning, Perceptual Grouping, Neuroscience",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=ElI9znK_eUz,https://openreview.net/forum?id=ElI9znK_eUz,"~Lakshmi_Narasimhan_Govindarajan3, ~Rex_G_Liu1, ~Drew_Linsley1, ~Alekh_Karkada_Ashok1, ~Max_Reuter1, ~Michael_Frank2, ~Thomas_Serre1",1660000000000.0,
cMEnMVvMw9,Token-Importance Guided Direct Preference Optimization,"Aligning Large Language Models (LLMs) with human preferences is crucial for safe and effective AI interactions. While popular methods like Direct Preference Optimization (DPO) have simplified alignment, they remain sensitive to data noise and overlook the differential importance of individual tokens. Existing token-level approaches often rely on probability prediction or simplistic weighting schemes to obtain token importance, which still cannot fully address these issues. To solve this problem, we propose the Token-Importance Guided Direct Preference Optimization (TI-DPO), a framework that achieves fine-grained semantic control through two synergistic innovations. 
First, we propose a novel hybrid weighting mechanism that combines gradient attribution with a Gaussian prior, ensuring both the accuracy and robustness of token importance scores. Second, we employ a triplet loss to provide structured guidance for the optimization, explicitly guiding model outputs to approach preferred responses and diverge from non-preferred ones. Experimental results show that TI-DPO achieves higher accuracy and stronger generative diversity, providing more stable and computationally efficient solutions compared with DPO and other RLHF methods.",,,"LLMs, RLHF, DPO, Human Preference Alignment, Token-lmportance, Triplet Loss","foundation or frontier models, including LLMs",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=cMEnMVvMw9,https://openreview.net/forum?id=cMEnMVvMw9,,,
5ZaoXB3MdP,R-WoM: Retrieval-augmented World Model For Computer-use Agents,"Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLM’s tendency to hallucination and their reliance on static training knowledge, which could lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models – future state prediction and reward estimation – through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs’ limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) com-
pared to baselines, with particular advantage in longer-horizon simulations.",,,"Large Language Model, Computer-use Agent, World Model","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=5ZaoXB3MdP,https://openreview.net/forum?id=5ZaoXB3MdP,,,
IBACinPJG5,MIRAGE: Modelling Interpretable Multivariate Time Series Forecasts with Actionable Ground Explanations,"Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Trans- formers. However, in critical situations like predicting a death in an ICU or sudden gaming overindulgence; an accurate prediction without a contributing evidence is irrelevant. It is important to have model driven Interpretability, allowing proactive comprehension of trajectory to an extremity; and an associated Explainability, al- lowing for preventive steps; e.g., controlling BP to avoid death, or nudging players to take breaks to prevent overplay. We introduce a novel deep neural network, MI- RAGE, which overcomes the inter-dependent challenges of—(a) temporally non- smooth data trajectories for interpretability; (b) highly multi-dimensional tempo- ral space for explainability; and (c) improving forecasting accuracy—all at once. MIRAGE: (i) achieves over 85% improvement on the MSE of the forecasts on the most relevant SOM-VAE based SOTA networks; and (ii) unravels the intricate multi-variate relationships and temporal trajectories contributing to any sudden movement to criticalities on temporally chaotic datasets.","Hussain Jagirdar, Rukma Ameet Talwadker, Aditya Pareek, Pulkit Agrawal, Tridib Mukherjee","hussain.jagirdar@games24x7.com, ~Rukma_Ameet_Talwadker1, aditya.pareek@games24x7.com, pulkit.agrawal@games24x7.com, ~Tridib_Mukherjee1","Representation Learning, Interpretable Representations, Explainability, Forecasting, Markov Models, LSTM, Attention Networks, Clustering","unsupervised, self-supervised, semi-supervised, and supervised representation learning",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=IBACinPJG5,https://openreview.net/forum?id=IBACinPJG5,,,
7QlJcWwd14,ReasonAudio: Semantic Reasoning and Temporal Synchrony in Video–Text-to-Audio Generation,"The rapid advancement of video-text-to-audio (VT2A) diffusion models has enabled unprecedented audio generation conditioned on video and text, yet two major challenges remain: following complex semantic descriptions and achieving robust audio–visual synchronization. In this work, we propose ReasonAudio, an MLLM-empowered flow-matching generative model with stronger semantic and robust temporal alignment. To enhance semantic understanding, we 1) address the scarcity of semantically rich tri-modal (video–text–audio) annotations by constructing VGGSound-Think, a dataset enriched with acoustic hints and audio–visual relation descriptions, and 2) leverage MLLMs to understand multimodal conditions (video and text) by introducing learnable queries that bridge understanding and generation components. To tackle temporal alignment, we employ preference optimization (Flow-DPO, Flow-RWR) with synchronization feedback, aligning generative models with visual synchrony preferences. Extensive experiments demonstrate that ReasonAudio achieves state-of-the-art performance in VT2A generation, with substantial improvements in both semantic alignment and temporal synchronization. Moreover, evaluations on VGGSound-Think show that our model excels at reasoning over acoustic hints and following descriptions of audio–visual relations (e.g., object interactions and on-/off-screen attribution). The demo page is available at https://ReasonAudio.github.io.",,,Multimodal Large Language Models; Video-to-Audio Generation; Preference Optimization;,"applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=7QlJcWwd14,https://openreview.net/forum?id=7QlJcWwd14,,,
DUfwD5yiN4,Exact Distributed Structure-Learning for Bayesian Networks,"Learning the structure of a Bayesian network is currently practical for only a limited number of variables. Existing distributed learning approaches approximate the true structure. We present an exact distributed structure-learning algorithm to find a P-map for a set of random variables. First, by using conditional independence, the variables are divided into sets $\X_1,\ldots,\X_I$ such that for each $\X_i$, the presence and absence of edges that are adjacent with any interior node (a node that is not in any other $\X_j, j\neq i$) can be correctly identified by learning the structure of $\X_i$ separately without using the information of the variables other than $\X_i$. Second, constraint or score-based structure learners are employed to learn the P-map of $\X_i$, in a decentralized way. Finally, the separately learned structures are appended by checking a conditional independence test on the boundary nodes (those that are in at least two $\X_i$'s). The result is proven to be a P-map. This approach allows for a significant reduction in computation time and opens the door for structure learning for a ``giant'' number of variables.","Hamid Kalantari, Mohammad Hossein Roohi, Pouria Ramazi","~Hamid_Kalantari1, ~Mohammad_Hossein_Roohi1, ~Pouria_Ramazi1","Bayesian networks, Causality, Structure learning, Distributed learning",learning theory,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=DUfwD5yiN4,https://openreview.net/forum?id=DUfwD5yiN4,,,
lYRkGZZi9D,Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching: With Insights into Other Permutation Search Methods,"Recently, Ainsworth et al. (2023) showed that using weight matching (WM) to minimize the $L^2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), where the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper analyzes LMC using WM, which is useful for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first empirically show that permutations found by WM do not significantly reduce the $L^2$ distance between two models, and the occurrence of LMC is not merely due to distance reduction by WM itself. We then demonstrate that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM primarily align the directions of singular vectors associated with large singular values across models. This alignment brings the singular vectors with large singular values, which determine the model's functionality, closer between the original and merged models, allowing the merged model to retain functionality similar to the original models, thereby satisfying LMC. This paper also analyzes activation matching (AM) in terms of singular vectors and finds that the principle of AM is likely the same as that of WM. Finally, we analyze the difference between WM and the straight-through estimator (STE), a dataset-dependent permutation search method, and show that WM can be more advantageous than STE in achieving LMC among three or more models.","Akira Ito, Masanori Yamada, Atsutoshi Kumagai","~Akira_Ito1, ~Masanori_Yamada1, ~Atsutoshi_Kumagai2","linear mode connectivity, deep learning, permutation symmetry","other topics in machine learning (i.e., none of the above)",ICLR 2025 Poster,2025,https://openreview.net/pdf?id=lYRkGZZi9D,https://openreview.net/forum?id=lYRkGZZi9D,,,
SIE9fNq8lk,Text2Grad: Reinforcement Learning from Natural Language Feedback,"Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, \name aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model’s policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback–annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available in the supplementary ZIP file.",,,"Natural Language Feedback, Fine-Grained Policy Optimization, Reinforcement Learning for Language Models",reinforcement learning,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=SIE9fNq8lk,https://openreview.net/forum?id=SIE9fNq8lk,,,
sVbFE6xz82,MFN: Metadata-Free Real-World Noisy Image Generation,"Real-world noise poses a significant challenge in signal processing, especially for denoising tasks.
Although end-to-end denoising approaches have achieved exceptional performance, they are constrained to scenarios with abundant noisy-clean image pairs, which can be technically challenging and resource-intensive to collect.
To address this issue, several generative methods have been developed to synthesize realistic noisy images from limited real-world datasets.
While prior studies require camera metadata during training or testing to handle various real-world noise, the absence of metadata or variations in the information across different capturing devices is common in real-world scenarios, such as medical or microscope imaging, which limits their applicability.
Thus, we aim to eliminate the need for explicit camera-related labels in both stages, enhancing applicability in real-world scenarios.
To achieve this, we propose a novel framework called the Metadata-Free Noise Model (MFN), which extracts prompt features that encode input noise characteristics and generates diverse noisy images that adhere to the distribution of the input noise.
Extensive experimental results demonstrate the superior performance of our model in real-world noise generation and denoising across various benchmark datasets.","Jaekyun Ko, Dongjin Kim, Guanghui Wang, Tae Hyun Kim","~Jaekyun_Ko1, ~Dongjin_Kim3, ~Guanghui_Wang8, ~Tae_Hyun_Kim2","real noise modeling, prompt learning, consistency models, low-level vision","applications to computer vision, audio, language, and other modalities",Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=sVbFE6xz82,https://openreview.net/forum?id=sVbFE6xz82,,,
JjMRdXPpKQ,Can Diffusion Models Disentangle? A Theoretical Perspective,"This paper introduces a novel theoretical framework to understand how diffusion models can learn disentangled representations under the assumption of an $\normltwo$ score approximation. We also provide sufficient conditions under which such representations are beneficial for domain adaptation. Our theory offers new insights into how existing diffusion models disentangle latent variables across general distributions and suggests strategies to enhance their disentanglement capabilities. To validate our theory, we perform experiments using both synthetic data generated from latent subspace models and real speech data for non-parallel voice conversion - a canonical disentanglement problem. Across various classification tasks, we found voice conversion-based adaptation methods achieve significant improvements in classification accuracy, demonstrating their effectiveness as domain adaptors. Code will be released upon acceptance.","Liming Wang, Yuan Gong, Marco Vilela, James R. Glass","~Liming_Wang3, ~Yuan_Gong3, ~Marco_Vilela1, ~James_R._Glass1","diffusion model, disentanglement, voice conversion",learning theory,ICLR 2025 Conference Withdrawn Submission,2025,https://openreview.net/pdf?id=JjMRdXPpKQ,https://openreview.net/forum?id=JjMRdXPpKQ,,,
uXBIfzhu9T,The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation,"Assessing causal effects in the presence of unobserved confounding is a challenging problem. Existing studies leveraged proxy variables or multiple treatments to adjust for the confounding bias. In particular, the latter approach attributes the impact on a single outcome to multiple treatments, allowing estimating latent variables for confounding control. Nevertheless, these methods primarily focus on a single outcome, whereas in many real-world scenarios, there is greater interest in studying the effects on multiple outcomes. Besides, these outcomes are often coupled with multiple treatments. Examples include the intensive care unit (ICU), where health providers evaluate the effectiveness of therapies on multiple health indicators. To accommodate these scenarios, we consider a new setting dubbed as *multiple treatments and multiple outcomes*. We then show that parallel studies of multiple outcomes involved in this setting can assist each other in causal identification, in the sense that we can exploit other treatments and outcomes as proxies for each treatment effect under study. We proceed with a causal discovery method that can effectively identify such proxies for causal estimation. The utility of our method is demonstrated in synthetic data and sepsis disease.","Yong Wu, Mingzhou Liu, Jing Yan, Yanwei Fu, Shouyan Wang, Yizhou Wang, Xinwei Sun","~Yong_Wu9, ~Mingzhou_Liu1, ~Jing_Yan2, ~Yanwei_Fu2, ~Shouyan_Wang1, ~Yizhou_Wang1, ~Xinwei_Sun1","multiple treatments, multiple outcomes, proximal causal leaning, hypothetical test",causal reasoning,ICLR 2024 Conference Withdrawn Submission,2024,https://openreview.net/pdf?id=uXBIfzhu9T,https://openreview.net/forum?id=uXBIfzhu9T,,,
RZ8esDBqMJ,A tale of two tails: Preferred and anti-preferred natural stimuli in visual cortex,"A fundamental quest in neuroscience is to find the preferred stimulus of a sensory neuron. This search lays the foundation for understanding how selectivity emerges in the primate visual stream---from simple edge-detecting neurons to highly-selective ""grandmother"" neurons---as well as for the architectures and activation functions of deep neural networks. The prevailing notion is that a visual neuron primarily responds to a single preferred visual feature like an oriented edge or object identity, resulting in a ""one-tailed"" distribution of responses to natural images. However, surprisingly, we instead find ""two-tailed"" response distributions of neurons in higher-order visual cortex (macaque V4), suggesting that V4 neurons have both preferred and anti-preferred stimuli. We ran further experiments to validate the existence of anti-preferred stimuli in V4. We find that these anti-preferred stimuli help to shape a neuron's tuning: Only a small number of preferred and anti-preferred images are needed to estimate the rest of a neuron's tuning. Moreover, in a psychophysics task, humans rely on anti-preferred images to interpret and predict V4 stimulus tuning; this was not the case for hidden units from a deep neural network. We find that the preferred and anti-preferred visual features, while clearly distinguishable for individual neurons, are not easily distinguishable across neurons. Thus, the V4 population seemingly encodes anti-preferred stimuli to double its capacitity for feature selectivity. To encourage future experiments searching for anti-preferred stimuli, we release a tool called ImageBeagle to efficiently ""hunt"" for a neuron's preferred and anti-preferred stimuli by traversing the nearest neighbor graph of 30 million natural images. Overall, we establish anti-preferred stimuli as an important encoding property of V4 neurons. Our work embarks on a new quest in neuroscience to search for anti-preferred stimuli along the visual stream as well as update our deep neural network models of visual cortex to account for the two-tailed response distributions of neurons.",,,"computational neuroscience, neuronal tuning, stimulus selectivity, higher-order visual cortex, human psychophysics, stimulus optimization, deep neural networks",applications to neuroscience & cognitive science,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=RZ8esDBqMJ,https://openreview.net/forum?id=RZ8esDBqMJ,,,
jRpD8VfGRf,Multi-interest Disentangled Representation Learning for Multimodal Recommendation,"In recent years, multimodal recommendation systems have been widely used in e-commerce and short video platforms. How to effectively utilize multimodal data and avoid the interference of multimodal noise information has become the key research direction of researchers. Many studies add multimodal data as auxiliary features to the model, which brings positive effects. Pictures, text and audio signals in short videos are more likely to attract users' interest than basic attributes. The user's multiple personalized interests largely determine the user's behavioral preferences. In order to effectively utilize user interest to improve model effect, We propose a new Multi-interest Disentangled Representation Learning method for multimodal recommendation (MIDR). Specifically, we first introduce the expected maximum to describe the relationship between interest and predicted target, and establish the optimization object based on multi-interest recommendation. Then, considering the relationship between user interest and multiple modalities, we introduce disentangled representation learning to extract modal sharing and modal specific interest representations. Furthermore, we introduce multi-interest contrast module to help model learning interest representation based on self-supervised learning. We conducted experiments on three real-world data sets, and our proposed MIDR outperformed other state-of-art models. The effectiveness of the disentangled interest representation module and interest contrast module was verified by the ablation experiment.","Wei Yang, Qingchen Yang, Yiqun Chen, Chi Lu","~Wei_Yang10, ~Qingchen_Yang1, ~Yiqun_Chen2, ~Chi_Lu2","Multiple Interests, Disentangled Representation, Multimodal Recommendation","unsupervised, self-supervised, semi-supervised, and supervised representation learning",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=jRpD8VfGRf,https://openreview.net/forum?id=jRpD8VfGRf,,,
hdcTQ4eEVJ,Multi-Objective Markov Games: Theoretic Foundations and Learning Algorithms,"In practical multi-agent systems, agents often have diverse objectives, which makes the system more complex, as each agent's performance across multiple criteria depends on the joint actions of all agents, creating intricate strategic trade-offs. To address this, we introduce the  Multi-Objective Markov Game (MOMG), a framework for multi-agent reinforcement learning with multiple objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary solution concept, where no agent can unilaterally improve one objective without sacrificing performance on another. We prove existence of PNE, and establish an equivalence between the PNE and the set of Nash Equilibria of MOMG's corresponding linearly scalarized games, enabling solutions of MOMG by transferring to a standard single-objective Markov game. However, we note that computing a PNE is theoretically and computationally challenging, thus we propose and study weaker but more tractable solution concepts. Building on these foundations, we develop online learning algorithm that identify a single solution to MOMGs. Furthermore, we propose a novel two-phase, preference-free algorithm that decouples exploration from planning. Our algorithm enables computation of a PNE for any given preference profile without collecting new samples, providing an efficient methodological characterization of the entire Pareto-Nash front.",,,"Multi-objective learning, Markov games, reinforcement leaning",reinforcement learning,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=hdcTQ4eEVJ,https://openreview.net/forum?id=hdcTQ4eEVJ,,,
ye3NrNrYOY,Temporal Causal Mechanism Transfer for Few-shot Action Recognition,"The goal of few-shot action recognition is to recognize actions in video sequences for which there exists only a few training samples. The challenge is to adapt a base model effectively and efficiently when the base and novel data have significant distributional disparities. To this end, we learn a model of a temporal causal mechanism from the base data by variational inference. When adapting the model by training on the novel data set we hold certain aspects of the causal mechanism fixed, updating only auxiliary variables and a classifier. During this adapation phase, we treat as invariant the time-delayed causal relations between latent causal variables and the mixing function that maps causal variables to action representations. Our experimental evaluations across standard action recognition datasets validate our hypothesis that our proposed method of Temporal Causal Mechanism Transfer (TCMT) enables efficient few-shot action recognition in video sequences with notable performance improvements over leading benchmarks.","Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellotti, Donglai Wei","~Yuke_Li1, ~Guangyi_Chen1, ~Ben_Abramowitz1, ~Stefano_Anzellotti1, ~Donglai_Wei1","few-shot action recognition, causal representation learning","representation learning for computer vision, audio, language, and other modalities",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=ye3NrNrYOY,https://openreview.net/forum?id=ye3NrNrYOY,,,
6Wl7-M2BC-,An Adaptive Policy to Employ Sharpness-Aware Minimization,"Sharpness-aware minimization (SAM), which searches for flat minima by min-max optimization, has been shown to be useful in improving model generalization. However, since each SAM update requires computing two gradients, its computational cost and training time are both doubled compared to standard empirical risk minimization (ERM). Recent state-of-the-arts reduce the fraction of SAM updates and thus accelerate SAM by switching between SAM and ERM updates randomly or periodically. In this paper, we design an adaptive policy to employ SAM based on the loss landscape geometry. Two efficient algorithms, AE-SAM and AE-LookSAM, are proposed. We theoretically show that AE-SAM has the same convergence rate as SAM. Experimental results on various datasets and architectures demonstrate the efficiency and effectiveness of the adaptive policy.","Weisen Jiang, Hansi Yang, Yu Zhang, James Kwok",,"Sharpness-aware minimization, model generalization, loss landscape",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=6Wl7-M2BC-,https://openreview.net/forum?id=6Wl7-M2BC-,"~Weisen_Jiang1, ~Hansi_Yang1, ~Yu_Zhang3, ~James_Kwok1",1660000000000.0,
ywAjQw-spmY,Formal Specifications from Natural Language,"We study the generalization abilities of language models when translating natural language into formal specifications with complex semantics. In particular, we fine-tune language models on three datasets consisting of English sentences and their corresponding formal representation: 1) regular expressions (regex), frequently used in programming and search; 2) First-order logic (FOL), commonly used in software verification and theorem proving; and 3) linear-time temporal logic (LTL), which forms the basis for industrial hardware specification languages. Our experiments show that, in these diverse domains, the language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions. Additionally, they achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions, with the benefits of being easy to access, efficient to fine-tune, and without a particular need for domain-specific reasoning.","Christopher Hahn, Frederik Schmitt, Julia Janice Tillman, Niklas Metzger, Julian Siber, Bernd Finkbeiner",,"language models, natural language, formal specifications, first-order logic, temporal logic, regular expressions",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=ywAjQw-spmY,https://openreview.net/forum?id=ywAjQw-spmY,"~Christopher_Hahn1, ~Frederik_Schmitt1, ~Julia_Janice_Tillman1, ~Niklas_Metzger1, ~Julian_Siber1, ~Bernd_Finkbeiner1",1660000000000.0,
iHQIacMKka,Risk-Sensitive Agent Compositions,"From software development to robot control, modern agentic systems decompose complex objectives into a sequence of subtasks and choose a set of specialized AI agents to complete them.
We formalize agentic workflows as directed acyclic graphs, called agent graphs, where edges represent AI agents and paths correspond to feasible compositions of agents.
Real-world deployment requires selecting agent compositions that not only maximize task success but also minimize violations of safety, fairness, and privacy requirements which demands a careful analysis of the low-probability (tail) behaviors of compositions of agents.
In this work, we consider risk minimization over the set of feasible agent compositions and seek to minimize the value-at-risk of the loss distribution of the agent composition where the loss quantifies violations of these requirements.
We introduce an efficient algorithm which traverses the agent graph and finds a near-optimal composition of agents.
It uses a dynamic programming approach to approximate the value-at-risk of agent compositions by exploiting a union bound.
Furthermore, we prove that the approximation is near-optimal asymptotically for a broad class of practical loss functions.
To evaluate our framework, we consider a suite of video game-like control benchmarks that require composing several agents trained with reinforcement learning and demonstrate our algorithm's effectiveness in approximating the value-at-risk and identifying the optimal agent composition.",,,"Agentic systems, agent composition, safety, risk-sensitive planning, planning","probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=iHQIacMKka,https://openreview.net/forum?id=iHQIacMKka,,,
COO51g41Q4,Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance,"The Vision Transformer (ViT) has emerged as a powerful architecture for various computer vision tasks. Nonetheless, this comes with substantially heavier computational costs than Convolutional Neural Networks (CNNs). The attention mechanism in ViTs, which integrates information from different image patches to the class token ([CLS]), renders traditional structured pruning methods used in CNNs unsuitable. To overcome this issue, we propose SynergisTic pAtch pRuning (STAR) that unifies intra-layer and inter-layer patch importance scoring. Specifically, our approach combines a) online evaluation of intra-layer importance for the [CLS] and b) offline evaluation of the inter-layer importance of each patch. The two importance scores are fused by minimizing a weighted average of Kullback-Leibler (KL) Divergences and patches are successively pruned at each layer by maintaining only the top-k most important ones. Unlike prior art that relies on manual selection of the pruning rates at each layer, we propose an automated method for selecting them based on offline-derived metrics. We also propose a variant that uses these rates as weighted percentile parameters (for the layer-wise normalized scores), thus leading to an alternate adaptive rate selection technique that is input-based. Extensive experiments demonstrate the significant acceleration of the inference with minimal performance degradation. For instance, on the ImageNet dataset, the pruned DeiT-Small reaches a throughput of 4,256 images/s, which is over 66\% higher than the much smaller (unpruned) DeiT-Tiny model, while having a substantially higher accuracy (+6.8\% Top-1 and +3.1\% Top-5).","Yuyao Zhang, Lan Wei, Nikolaos Freris","~Yuyao_Zhang4, ~Lan_Wei2, ~Nikolaos_Freris2","Vision Transformers, Model compression, Dynamic pruning","representation learning for computer vision, audio, language, and other modalities",ICLR 2024 poster,2024,https://openreview.net/pdf?id=COO51g41Q4,https://openreview.net/forum?id=COO51g41Q4,,,
_tfLpF9mFiq,No Pairs Left Behind: Improving Metric Learning with Regularized Triplet Objective,"We propose a novel formulation of the triplet objective function that improves metric learning without additional sample mining or overhead costs. Our approach aims to explicitly regularize the distance between the positive and negative samples in a triplet with respect to the anchor-negative distance. As an initial validation, we show that our method (called No Pairs Left Behind [NPLB]) improves upon the traditional and current state-of-the-art triplet objective formulations on standard benchmark datasets. To show the effectiveness and potentials of NPLB on real-world complex data, we evaluate our approach on a large-scale healthcare dataset (UK Biobank), demonstrating that the embeddings learned by our model significantly outperform all other current representations on tested downstream tasks. Additionally, we provide a new model-agnostic single-time health risk definition that, when used in tandem with the learned representations, achieves the most accurate prediction of a patient's future health complications. Our results indicate that NPLB is a simple, yet effective framework for improving existing deep metric learning models, showcasing the potential implications of deep metric learning in more complex applications, especially in the biological and healthcare domains.","A. Ali Heydari, Naghmeh Rezaei, Daniel McDuff, Javier L Prieto",,"Deep Metric Learning, Representation Learning, Machine Learning for Healthcare, Triplet Loss",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=_tfLpF9mFiq,https://openreview.net/forum?id=_tfLpF9mFiq,"~A._Ali_Heydari1, ~Naghmeh_Rezaei1, ~Daniel_McDuff1, ~Javier_L_Prieto1",1660000000000.0,
zXM0b4hi5_B,On the relation between statistical learning and perceptual distances,"It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure.","Alexander Hepburn, Valero Laparra, Raul Santos-Rodriguez, Jona Ballé, Jesus Malo",,,,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=zXM0b4hi5_B,https://openreview.net/forum?id=zXM0b4hi5_B,"~Alexander_Hepburn2, ~Valero_Laparra1, ~Raul_Santos-Rodriguez1, ~Jona_Ballé1, ~Jesus_Malo1",1630000000000.0,
K1mcPiDdOJ,Conditional Information Bottleneck Approach for Time Series Imputation,"Time series imputation presents a significant challenge because it requires capturing the underlying temporal dynamics from partially observed time series data. Among the recent successes of imputation methods based on generative models, the information bottleneck (IB) framework offers a well-suited theoretical foundation for multiple imputations, allowing us to account for the uncertainty associated with the imputed values. However, directly applying the IB framework to time series data without considering their temporal context can lead to a substantial loss of temporal dependencies, which, in turn, can degrade the overall imputation performance. To address such a challenge, we propose a novel conditional information bottleneck (CIB) approach for time series imputation, which aims to mitigate the potentially negative consequences of the regularization constraint by focusing on reducing the redundant information conditioned on the temporal context. We provide a theoretical analysis of its effect by adapting variational decomposition. We use the resulting insight and propose a novel deep learning method that can approximately achieve the proposed CIB objective for time series imputation as a combination of evidence lower bound and novel temporal kernel-enhanced contrastive optimization. Our experiments, conducted on multiple real-world datasets, consistently demonstrate that our method significantly improves imputation performance (including both interpolation and extrapolation), and also enhances classification performance based on the imputed values.","MinGyu Choi, Changhee Lee","~MinGyu_Choi1, ~Changhee_Lee1",time series imputation; time series interpolation; information bottleneck,"general machine learning (i.e., none of the above)",ICLR 2024 poster,2024,https://openreview.net/pdf?id=K1mcPiDdOJ,https://openreview.net/forum?id=K1mcPiDdOJ,,,
SJl3h2EYvS,CLAREL: classification via retrieval loss for zero-shot learning,"We address the problem of learning fine-grained cross-modal representations. We propose an instance-based deep metric learning approach in joint visual and textual space. The key novelty of this paper is that it shows that using per-image semantic supervision leads to substantial improvement in zero-shot performance over using class-only supervision. On top of that, we provide a probabilistic justification for a metric rescaling approach that solves a very common problem in the generalized zero-shot learning setting, i.e., classifying test images from unseen classes as one of the classes seen during training. We evaluate our approach on two fine-grained zero-shot learning datasets: CUB and FLOWERS. We find that on the generalized zero-shot classification task CLAREL consistently outperforms the existing approaches on both datasets.","Boris N. Oreshkin, Negar Rostamzadeh, Pedro O. Pinheiro, Christopher Pal",,"zero-shot learning, representation learning, fine-grained classification",,ICLR.cc/2020/Conference,2020,https://openreview.net/pdf?id=SJl3h2EYvS,https://openreview.net/forum?id=SJl3h2EYvS,"boris@elementai.com, negar@elementai.com, pedro@elementai.com, christopher.pal@elementai.com",1570000000000.0,
PBXhT2pdvs,EWE: An Agentic Framework for Extreme Weather Analysis,"Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and  offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.",,,"Extreme Weather Analysis, AI for Science","applications to physical sciences (physics, chemistry, biology, etc.)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=PBXhT2pdvs,https://openreview.net/forum?id=PBXhT2pdvs,,,
4RwkbKZhGV,A Time-Consistency Curriculum for Learning from Instance-Dependent Noisy Labels,"Many machine learning algorithms are known to be fragile on simple instance-independent noisy labels. However, noisy labels in real-world data are more devastating since they are produced by more complicated mechanisms in an instance-dependent manner. In this paper, we target this practical challenge of  \textit{Instance-Dependent Noisy Labels} by jointly training 
(1) a model reversely engineering the noise generating mechanism, which produces an \textit{instance-dependent mapping} between the clean label posterior and the observed noisy label; and (2) a robust classifier that produces clean label posteriors. Compared to previous methods, the former model is novel and enables end-to-end learning of the latter directly from noisy labels. An extensive empirical study indicates that the time-consistency of data is critical to the success of training both models and motivates us to develop a curriculum selecting training data based on their dynamics on the two models' outputs over the course of training. We show that the curriculum-selected data provide both clean labels and high-quality input-output pairs for training the two models. Therefore, it leads to promising and robust classification performance even in notably challenging settings of instance-dependent noisy labels where many SoTA methods could easily fail. Extensive experimental comparisons and ablation studies further demonstrate the advantages and significance of the time-consistency curriculum in learning from instance-dependent noisy labels on multiple benchmark datasets.","Songhua Wu, Tianyi Zhou, Yuxuan Du, Jun Yu, Bo Han, Tongliang Liu",,,,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=4RwkbKZhGV,https://openreview.net/forum?id=4RwkbKZhGV,"~Songhua_Wu1, ~Tianyi_Zhou1, ~Yuxuan_Du2, ~Jun_Yu3, ~Bo_Han1, ~Tongliang_Liu1",1660000000000.0,
FIrQfNSOoTr,Instance-wise Batch Label Restoration via Gradients in Federated Learning,"Gradient inversion attacks have posed a serious threat to the privacy of federated learning. The attacks search for the optimal pair of input and label best matching the shared gradients and the search space of the attacks can be reduced by pre-restoring labels. Recently, label restoration technique allows for the extraction of labels from gradients analytically, but even the state-of-the-art remains limited to identify the presence of categories (i.e., the class-wise label restoration). This work considers the more real-world settings, where there are multiple instances of each class in a training batch. An analytic method is proposed to perform instance-wise batch label restoration from only the gradient of the final layer. On the basis of the approximate recovered class-wise embeddings and post-softmax probabilities, we establish linear equations of the gradients, probabilities and labels to derive the Number of Instances (NoI) per class by the Moore-Penrose pseudoinverse algorithm. Our experimental evaluations reach over 99% Label existence Accuracy (LeAcc) and exceed 96% Label number Accuracy (LnAcc) in most cases on three image datasets and four classification models. The two metrics are used to evaluate class-wise and instance-wise label restoration accuracy, respectively. And the recovery is made feasible even with a batch size of 4096 and partially negative activations (e.g., Leaky ReLU and Swish). Furthermore, we demonstrate that our method facilitates the existing gradient inversion attacks by exploiting the recovered labels, with an increase of 6-7 in PSNR on both MNIST and CIFAR100. Our code is
available at https://github.com/BUAA-CST/iLRG.","Kailang Ma, Yu Sun, Jian Cui, Dawei Li, Zhenyu Guan, Jianwei Liu",,"federated learning, batch label restoration, gradient inversion attack.",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=FIrQfNSOoTr,https://openreview.net/forum?id=FIrQfNSOoTr,"~Kailang_Ma1, sunyv@buaa.edu.cn, cuijianw@buaa.edu.cn, lidawei@buaa.edu.cn, ~Zhenyu_Guan1, liujianwei@buaa.edu.cn",1660000000000.0,
49jkevjF6x,Multlingual Abstractive Event Extraction for the Real World,"Event extraction (EE) is a valuable tool for making sense of large amounts of unstructured data, with a wide range of real-world applications, from studying disease outbreaks to monitoring political violence. Current EE systems rely on cumbersome mention-level annotations, and event arguments are frequently restricted to ungrounded spans of text, which hinders the aggregation and analysis of extracted events. In this paper, we define a new abstractive event extraction (AEE) task that moves away from the surface form and instead requires a deeper
wholistic understanding of the input text. To support research in this direction, we release a new multilingual, expert-annotated event dataset called Lemonade, which covers 16 languages, including several for which no event dataset currently exists. Lemonade has 41,148 events, and is based on the Armed Conflict Location and Event Data Project, which has been collecting and coding data on political violence around the globe for over a decade. We introduce a novel zero-shot AEE system Zest that achieves a score of 57.2% F1 on Lemonade. With our supervised model that achieves 71.6% F1, they represent strong baselines for this new dataset.","Sina Semnani, Pingyue Zhang, Wanyue Zhai, Haozhuo Li, Ryan Beauchamp, Trey Billing, Katayoun Kishi, Manling Li, Monica Lam","~Sina_Semnani1, ~Pingyue_Zhang1, ~Wanyue_Zhai1, ~Haozhuo_Li1, ~Ryan_Beauchamp1, ~Trey_Billing1, ~Katayoun_Kishi1, ~Manling_Li1, ~Monica_Lam1","dataset, event extraction, multilingual, zero-shot, entity linking",datasets and benchmarks,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=49jkevjF6x,https://openreview.net/forum?id=49jkevjF6x,,,
EcbUaXJb9s,MARWA: Multi-agent retrieval-augmented framework for reliable bioinformatics workflow automation,"The rapid growth of multi-omics data has driven the expansion of bioinformatics analysis tools. Common bioinformatics tasks often rely on workflows, which link multiple tools into structured pipelines for reproducibility and scalability. Yet, building workflows manually is slow and error-prone, motivating efforts toward automation. However, bioinformatics workflow automation remains difficult due to the need to clarify vague analytical objectives, coordinate heterogeneous tools, and generate intricate tool commands. Despite the potential of large language models (LLMs) to aid bioinformatics workflow recommendation through advanced semantic understanding and logical reasoning, current agent frameworks often rely on one-shot generation, weak tool retrieval solution, and limited evaluation scheme, resulting in fragile workflow automation. We propose MARWA, a Multi-Agent Retrieval-augmented framework for reliable bioinformatics Workflow Automation. The framework emphasizes a step-by-step generation process with error handling at each stage to ensure robustness. We introduce a retrieval-augmented framework to strengthen tool command accuracy, which incorporates multi-perspective LLM-augmented descriptions and employs contrastive learning. We further design a two-stage evaluation framework, combining expert-verified execution on 40 curated tasks with large-scale benchmarking on 2,270 tasks using LLM-based evaluation. Our experiments demonstrate that MARWA consistently outperforms baselines in pass rate, workflow quality and scalability. Our work provides a foundation for trustworthy bioinformatics workflow automation. Project Page: https://anonymous.4open.science/r/MARWA-7D30.",,,Bioinformatics; Workflow Automation; Multi-Agent Systems; Retrieval-Augmented Generation; Large Language Models,"applications to physical sciences (physics, chemistry, biology, etc.)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=EcbUaXJb9s,https://openreview.net/forum?id=EcbUaXJb9s,,,
ideKAUWvFE,OSCAR: Online Soft Compression for RAG,"Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge, leading to improved accuracy and relevance. However, scaling RAG pipelines remains computationally expensive as context length grows. On one hand, hard compression methods have recently proposed to prune the retrieved text on-the-fly with a limited compression ration. On the other hand, soft compression method performs a costly offline compression thanks a dedicated LLM but with a higher compression rate. In this paper, we introduce OSCAR, a novel query-dependent online soft compression method for RAG. OSCAR bridges the gap between online hard and offline soft compression methods, bringing the best of both: OSCAR dynamically compresses retrieved information at inference time, eliminating storage overhead and enabling higher compression rates than existing methods. Our experiments demonstrate state-of-the-art performance with a 2-5x speed-up in inference and minimal, if any, accuracy loss, for LLMs ranging from 1B to 24B parameters.",,,"RAG, Compression, Embedding, Efficiency, Question Answering","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=ideKAUWvFE,https://openreview.net/forum?id=ideKAUWvFE,,,
cEyj6ewRFZ,Nonparametric Teaching for Sequential Property Learners,"Determining the properties of sequence-structured data, e.g., the sentiment of a text, fundamentally requires learning the implicit relationship that maps sequences to their corresponding properties. This learning process is often expensive for sequential property learners like Recurrent Neural Networks (RNNs). To tackle this, we introduce a paradigm called **Re**current **N**eural **T**eaching (ReNT), which reinterprets the learning process through a novel nonparametric teaching lens. Specifically, the latter provides a theoretical framework for teaching implicitly defined (i.e., nonparametric) mappings via example selection. Such an implicit mapping is realized by a dense set of sequence-property pairs, with the ReNT teacher selecting a subset of them to facilitate faster convergence in RNN training. By analytically investigating the effect of sequence order on parameter-based gradient descent during training, and recasting the evolution of RNNs—driven by parameter updates—through functional gradient descent in nonparametric teaching, we reveal *for the first time* that teaching sequential property learners (i.e., RNNs) is consistent with teaching order-aware nonparametric learners. These new findings readily prompt ReNT to improve the learning efficiency of the sequential property learner, achieving substantial cuts in training time for sequence-level (-32.77% to -46.39%) and element-level (-36% to -39.17%) tasks, while still preserving its generalization performance.",,,"Nonparametric Teaching, Sequential Property Learning, Functional Gradient Descent","other topics in machine learning (i.e., none of the above)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=cEyj6ewRFZ,https://openreview.net/forum?id=cEyj6ewRFZ,,,
CgkAGcp9lk,Compositional Search of Stable Crystalline Structures in Multi-Component Alloys Using Generative Diffusion Models,"Exploring the vast composition space of multi-component alloys presents a challenging task for both ab initio (first principles) and experimental methods due to the time-consuming procedures involved. This ultimately impedes the discovery of novel, stable materials that may display exceptional properties. Here, the Crystal Diffusion Variational Autoencoder (CDVAE) model is adapted to characterize the stable compositions of a well studied multi-component alloy, NiFeCr, with two distinct crystalline phases known to be stable across its compositional space. To this end, novel extensions to CDVAE were proposed, enhancing the model’s ability to reconstruct configurations from their latent space within the test set by approximately 30% . A fact that increases a model’s probability of discovering new materials when dealing with various crystalline structures. Afterwards, the new model is applied for materials generation, demonstrating excellent agreement in identifying stable configurations within the ternary phase space when compared to first principles data. Finally, a computationally efficient framework for inverse design is proposed, employing Molecular Dynamics (MD) simulations of multi- component alloys with reliable interatomic potentials, enabling the optimization of materials property across the phase space.","Grzegorz Kaszuba, Amirhossein Naghdi Dorabati, Stefanos Papanikolaou, Andrzej Jaszkiewicz, Piotr Sankowski","~Grzegorz_Kaszuba1, ~Amirhossein_Naghdi_Dorabati1, ~Stefanos_Papanikolaou2, andrzej.jaszkiewicz@put.poznan.pl, ~Piotr_Sankowski1","Multi-Component Alloys, Generative Diffusion Models, Composition Search, Inverse Design",generative models,Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=CgkAGcp9lk,https://openreview.net/forum?id=CgkAGcp9lk,,,
SkxWnkStvS,Searching for Stage-wise Neural Graphs In the Limit,"Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet.","Xin Zhou, Dejing Dou, Boyang Li",,"neural architecture search, graphon, random graphs",,ICLR.cc/2020/Conference,2020,https://openreview.net/pdf?id=SkxWnkStvS,https://openreview.net/forum?id=SkxWnkStvS,"chow459@gmail.com, doudejing@baidu.com, libo0001@gmail.com",1570000000000.0,
dTkqaCKLPp,SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation,"Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model's ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations.","Song Duong, Florian Le Bronnec, Alexandre Allauzen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari","~Song_Duong1, ~Florian_Le_Bronnec1, ~Alexandre_Allauzen1, ~Vincent_Guigue1, ~Alberto_Lumbreras1, ~Laure_Soulier1, ~Patrick_Gallinari2","faithfulness, hallucination, conditional text generation, natural language processing, large language models","applications to computer vision, audio, language, and other modalities",ICLR 2025 Poster,2025,https://openreview.net/pdf?id=dTkqaCKLPp,https://openreview.net/forum?id=dTkqaCKLPp,,,
yrPiUpkXQh,Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization,"Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and subjective variations among expert annotators. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generating diverse segmentations that lack expert specificity or producing personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized.",,,probabilistic modeling,"applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=yrPiUpkXQh,https://openreview.net/forum?id=yrPiUpkXQh,,,
PRR120c01e,Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations,"Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local
Verifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful
to decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies.",,,"LLM-as-a-Judge, Interpretability, Global Explanations, Local Explanations",interpretability and explainable AI,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=PRR120c01e,https://openreview.net/forum?id=PRR120c01e,,,
gXfKPj4s7C,Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection,"Point cloud-based open-vocabulary 3D object detection aims to detect 3D categories that do not have ground-truth annotations in the training set. It is extremely challenging because of the limited data and annotations (bounding boxes with class labels or text descriptions) of 3D scenes. Previous approaches leverage large-scale richly-annotated image datasets as a bridge between 3D and category semantics but require an extra alignment process between 2D images and 3D points, limiting the open-vocabulary ability of 3D detectors. Instead of leveraging 2D images, we propose Object2Scene, the first approach that leverages large-scale large-vocabulary 3D object datasets to augment existing 3D scene datasets for open-vocabulary 3D object detection. Object2Scene inserts objects from different sources into 3D scenes to enrich the vocabulary of 3D scene datasets and generates text descriptions for the newly inserted objects. We further introduce a framework that unifies 3D detection and visual grounding, named L3Det, and propose a cross-domain object-level contrastive learning approach to mitigate the domain gap between 3D objects from different datasets. Extensive experiments on existing open-vocabulary 3D object detection benchmarks show that Object2Scene obtains superior performance over existing methods. We further verify the effectiveness of Object2Scene on a new benchmark OV-ScanNet-200, by holding out all rare categories as novel categories not seen during training.","Chenming Zhu, Wenwei Zhang, Tai Wang, Xihui Liu, Kai Chen","~Chenming_Zhu1, ~Wenwei_Zhang1, ~Tai_Wang2, ~Xihui_Liu1, ~Kai_Chen4","open vocabulary 3d detection, 3d detection, contrastive learning","representation learning for computer vision, audio, language, and other modalities",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=gXfKPj4s7C,https://openreview.net/forum?id=gXfKPj4s7C,,,
GBfYgjOfSe,Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms,"Building a generalist model for user interface (UI) understanding is challenging due to various foundational issues, such as platform diversity, resolution variation, and data limitation. In this paper, we introduce Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI understanding across a wide range of platforms, including iPhone, Android, iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI 2 introduces three key innovations: support for multiple platform types, high-resolution perception through adaptive scaling, and advanced task training data generation powered by GPT-4o with set-of-mark visual prompting. These advancements enable Ferret-UI 2 to perform complex, user-centered interactions, making it highly versatile and adaptable for the expanding diversity of platform ecosystems. Extensive empirical experiments on referring, grounding, user-centric advanced tasks (comprising 9 subtasks $\times$ 5 platforms), GUIDE next-action prediction dataset, and GUI-World multi-platform benchmark demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also shows strong cross-platform transfer capabilities.","Zhangheng LI, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeffrey Nichols, Yinfei Yang, Zhe Gan","~Zhangheng_LI2, ~Keen_You1, ~Haotian_Zhang3, ~Di_Feng5, ~Harsh_Agrawal1, ~Xiujun_Li1, ~Mohana_Prasad_Sathya_Moorthy1, ~Jeffrey_Nichols1, ~Yinfei_Yang1, ~Zhe_Gan1","Multimodal LLM, UI Understanding","foundation or frontier models, including LLMs",ICLR 2025 Poster,2025,https://openreview.net/pdf?id=GBfYgjOfSe,https://openreview.net/forum?id=GBfYgjOfSe,,,
1QaTq2NXD6,ACTIVE-o3 : Empowering MLLMs with Active Perception via Pure Reinforcement Learning,"Active vision, also known as active perception, refers to actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. With the rise of Multimodal Large Language Models (MLLMs) as central planners in robotic systems, the lack of methods for equipping MLLMs with active perception has become a key gap. We first provide a systematic definition of MLLM-based active perception tasks and show that GPT-o3's zoom-in strategy can be viewed as a special case, though it suffers from low efficiency and inaccurate region selection. To address these issues, we propose Active-o3, a reinforcement learning framework built on GRPO that equips MLLMs with active perception capabilities. Leveraging a modular sensing-action design and a dual-form reward, Active-o3 autonomously learns efficient and stable region selection strategies without explicit supervision. We further establish a comprehensive benchmark covering both open-world tasks (small/dense-object grounding) and domain-specific scenarios (remote sensing, autonomous driving, interactive segmentation). Experimental results demonstrate that Active-o3 significantly enhances active perception capabilities compared to Qwen2.5-VL-CoT. Moreover, we show that our RL framework not only preserves the model’s general understanding ability but can also serve as a proxy task for leveraging perception data, further improving performance on benchmarks such as RealWorldQA. We hope that our work can provide a simple codebase and unified evaluation protocol to facilitate future research on active perception with MLLMs.",,,"vlm, RL, reasoning, MLLM","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=1QaTq2NXD6,https://openreview.net/forum?id=1QaTq2NXD6,,,
Dk7QQp8jHEo,Batch Multivalid Conformal Prediction,"We develop  fast distribution-free conformal prediction algorithms for obtaining multivalid coverage on exchangeable data in the batch setting. Multivalid coverage guarantees are stronger than marginal coverage guarantees in two ways: (1) They hold even conditional on group membership---that is, the target coverage level $1-\alpha$ holds conditionally on membership in each of an arbitrary (potentially intersecting) group in a finite collection $\mathcal{G}$ of regions in the feature space. (2) They hold even conditional on the value of the threshold used to produce the prediction set on a given example. In fact multivalid coverage guarantees hold even when conditioning on group membership and threshold value simultaneously.

We give two algorithms: both take as input an arbitrary non-conformity score and an arbitrary collection of possibly intersecting groups $\mathcal{G}$, and then can equip arbitrary black-box predictors with prediction sets.  Our first algorithm is a direct extension of quantile regression, needs to solve only a single convex minimization problem, and produces an estimator which has group-conditional guarantees for each group in $\mathcal{G}$. Our second algorithm is iterative, and gives the full guarantees of multivalid conformal prediction: prediction sets that are valid conditionally both on group membership and non-conformity threshold. We evaluate the performance of both of our algorithms in an extensive set of experiments.","Christopher Jung, Georgy Noarov, Ramya Ramalingam, Aaron Roth",,"Conformal prediction, multicalibration, uncertainty quantification",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=Dk7QQp8jHEo,https://openreview.net/forum?id=Dk7QQp8jHEo,"~Christopher_Jung1, ~Georgy_Noarov1, ~Ramya_Ramalingam1, ~Aaron_Roth1",1660000000000.0,
TvGPP8i18S,MELODI: Exploring Memory Compression for Long Contexts,"We present MELODI, a novel memory architecture designed to efficiently process long documents using short context windows. The key principle behind MELODI is to represent short-term and long-term memory as a hierarchical compression scheme across both transformer layers and context windows. Specifically, the short-term memory is achieved through recurrent compression of context windows across multiple layers, ensuring smooth transitions between windows. In contrast, the long-term memory performs further compression within a single middle layer and aggregates information across context windows, effectively consolidating crucial information from the entire history. Compared to a strong baseline - the Memorizing Transformer employing dense attention over a large long-term memory (64K key-value pairs) - our method demonstrates superior performance on various long-context datasets while remarkably reducing the memory footprint by a factor of 8.","Yinpeng Chen, DeLesley Hutchins, Aren Jansen, Andrey Zhmoginov, David Racz, Jesper Sparre Andersen","~Yinpeng_Chen1, ~DeLesley_Hutchins1, ~Aren_Jansen2, ~Andrey_Zhmoginov1, ~David_Racz1, ~Jesper_Sparre_Andersen1","Memory, Compression, Long Context","applications to computer vision, audio, language, and other modalities",ICLR 2025 Poster,2025,https://openreview.net/pdf?id=TvGPP8i18S,https://openreview.net/forum?id=TvGPP8i18S,,,
YlDHsGqjCO,Perishable Online Inventory Control with Context-Aware Demand Distributions,"We study the online contextual inventory control problem with perishable goods. In this work, we propose and consider a more realistic---and more challenging---setting where both the expected demand and the (residual) noise distribution depend on the observable features. Surprisingly, little is known when the noise is context-dependent, which captures the heteroskedastic uncertainty in demand that is important in inventory control. The optimal inventory quantity in this general setting is no longer a linear function of features (unlike the case when the expected demand is linear and the noise is i.i.d.), making online gradient descent---the gold standard therein---inapplicable. We first propose an algorithm that achieves the near-optimal regret $\widetilde{O}(\sqrt{d T}+T^{\frac{p+1}{p+2}})$ under linear expected demand and context-aware noise. Here $d$ is the feature dimension, and $p \leq d$ is an underlying dimension that captures the intrinsic complexity of the noise distribution. When the expected demand is nonlinear, we propose to use neural networks to capture the nonlinearity, and prove a regret bound $\widetilde{O}(\sqrt{\alpha T}+T^{\frac{p+1}{p+2}})$ under over-parameterized networks, where $\alpha$ depends on the nonlinear demand complexity and the network architecture. Additionally, under mild regularity conditions on the noise, the exponential factor $T^{\frac{p+1}{p+2}}$ in these regret bounds is improved to $p\sqrt{T}$. Finally, we provide a matching minimax lower bound $\Omega(\sqrt{d T}+T^{\frac{p+1}{p+2}})$ under linear expected demand. To our best knowledge, our results provide the first minimax optimal characterization for online inventory control with context-dependent noise and the first theoretical guarantees when the expected demand is nonlinear in features.",,,"online learning, inventory control, kernel regression, contextual bandits, neural bandits",learning theory,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=YlDHsGqjCO,https://openreview.net/forum?id=YlDHsGqjCO,,,
9OoFFWDPDQ,Delving into the Openness of CLIP,"Contrastive Language-Image Pre-training (CLIP) has demonstrated great potential in realizing open-vocabulary visual recognition in a matching style, due to its holistic use of natural language supervision that covers unconstrained real-world visual concepts. However, it is, in turn, also difficult to evaluate and analyze the openness of CLIP-like models, since they are in theory open to any vocabulary but the actual accuracy varies. To address the insufficiency of conventional studies on openness, we resort to an incremental perspective and define the extensibility, which essentially approximates the model's ability to deal with new visual concepts, by evaluating openness through vocabulary expansions. Our evaluation based on extensibility shows that CLIP-like models are hardly truly open and their performances degrade as the vocabulary expands to different degrees. Further analysis reveals that the over-estimation of openness is not because CLIP-like models fail to capture the general similarity of image and text features of novel visual concepts, but because of the confusion among competing text features, that is, they are not stable with respect to the vocabulary. In light of this, we propose to improve the openness of CLIP in feature space by enforcing the distinguishability of text features. Our method retrieves relevant texts from the pre-training corpus to enhance prompts for inference, which boosts the extensibility and stability of CLIP even without fine-tuning.","Shuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, Xu Sun",,"Contrastive Language-Image Pre-training, CLIP, Openness, Vision-and-Language",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=9OoFFWDPDQ,https://openreview.net/forum?id=9OoFFWDPDQ,"~Shuhuai_Ren1, ~Lei_Li14, ~Xuancheng_Ren1, ~Guangxiang_Zhao3, ~Xu_Sun1",1660000000000.0,
ICtXoEuZEV,Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training,"While large language models (LLMs) have demonstrated remarkable capabilities in language modeling, recent studies reveal that they often fail on out-of-distribution (OOD) samples due to spurious correlations acquired during pre-training. Here, we aim to mitigate such spurious correlations through causality-aware post-training (CAPT). By decomposing a biased prediction into two unbiased steps, known as \textit{event estimation} and \textit{event intervention}, we reduce LLMs' pre-training biases without incurring additional fine-tuning biases, thus enhancing the model's generalization ability. Experiments on the formal causal inference benchmark CLadder and the logical reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with CAPT can outperform both traditional SFT and larger LLMs on in-distribution (ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the effectiveness and sample efficiency of CAPT.",,,"LLM, fine-tuning, spurious correlations, causal inference","foundation or frontier models, including LLMs",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=ICtXoEuZEV,https://openreview.net/forum?id=ICtXoEuZEV,,,
2wFXD2upSQ,A Demon at Work: Leveraging Neuron Death for Efficient Neural Network Pruning,"When training deep neural networks, the phenomenon of ""dying neurons"" —units that become inactive and output zero throughout training—has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss, particularly in continual learning scenarios. In this paper, we reassess this phenomenon through the lens of network sparsity and pruning. By systematically exploring the influence of various hyperparameter configurations on the occurrence of dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce ""Demon's Pruning"" (DemP), a method that controls the proliferation of dead neurons, dynamically sparsifying neural networks as training progresses. Remarkably, our approach, characterized by its simplicity and broad applicability,  outperforms existing structured pruning techniques, while achieving results comparable to prevalent unstructured pruning methods. These findings pave the way for leveraging dying neurons as a valuable resource for efficient model compression and optimization.","Simon Dufort-Labbé, Pierluca D'Oro, Evgenii Nikishin, Razvan Pascanu, Pierre-Luc Bacon, Aristide Baratin","~Simon_Dufort-Labbé1, ~Pierluca_D'Oro1, ~Evgenii_Nikishin1, ~Razvan_Pascanu1, ~Pierre-Luc_Bacon1, ~Aristide_Baratin1","Pruning, Sparsity, Deep Learning, Regularization, Model Compression","unsupervised, self-supervised, semi-supervised, and supervised representation learning",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=2wFXD2upSQ,https://openreview.net/forum?id=2wFXD2upSQ,,,
Jnspzp-oIZE,Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs,"A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs).  Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods.","Pim De Haan, Maurice Weiler, Taco Cohen, Max Welling",,"symmetry, equivariance, mesh, geometric, convolution",,ICLR.cc/2021/Conference,2021,https://openreview.net/pdf?id=Jnspzp-oIZE,https://openreview.net/forum?id=Jnspzp-oIZE,"~Pim_De_Haan1, ~Maurice_Weiler1, ~Taco_Cohen1, ~Max_Welling1",1600000000000.0,
BydD1vNMCV,Statistical Inference for Deep Learning via Stochastic Modeling,"Deep learning has revolutionized big data analysis in modern data science, however, how to make statistical inference for deep neural networks remains largely unclear. To this end, we explore a stochastic 
variant of the deep neural network known as the stochastic neural network (StoNet). Firstly, we show that the StoNet falls into the framework of statistical modeling. It not only enables us to address fundamental issues in deep learning, such as structure interpretability and uncertainty quantification, but also provides with us a platform for transferring the theory and methods developed for linear models to the realm of deep learning. Specifically, we show how the sparse learning theory with the Lasso penalty can be adapted to deep neural networks (DNNs) from linear models; establish that the sparse StoNet is consistent in network structure selection; and provides a recursive method to quantify the prediction uncertainty for the Stonet. Furthermore, we extend this result to the DNN by its asymptotic equivalence with the StoNet, showing that consistent sparse deep learning can be obtained by training a DNN with an appropriate Lasso penalty. Additionally, we propose to remodel the last hidden layer output and the target output of a well-trained DNN model using a StoNet on the validation dataset, and then assess the prediction uncertainty of the DNN model via the Stonet. The proposed method has been compared with conformal inference on extensive examples, and numerical results suggests its superiority.","Yan Sun, Faming Liang","~Yan_Sun6, ~Faming_Liang1","Stochastic neural network, uncertainty quantification, nonlinear variable selection, stochastic gradient MCMC, imputation regularized-optimization","probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=BydD1vNMCV,https://openreview.net/forum?id=BydD1vNMCV,,,
KmphHE92wU,"Towards Stable, Globally Expressive Graph Representations with Laplacian Eigenvectors","Graph neural networks (GNNs) have achieved remarkable success in a variety of machine learning tasks over graph data. Existing GNNs usually rely on message passing, i.e., computing node representations by gathering information from the neighborhood, to build their underlying computational graphs. Such an approach has been shown fairly limited in expressive power, and often fails to capture global characteristics of graphs. To overcome the issue, a popular solution is to use Laplacian eigenvectors as additional node features, as they are known to contain global positional information of nodes, and can serve as extra node identifiers aiding GNNs to separate structurally similar nodes. Since eigenvectors naturally come with symmetries---namely, $O(p)$-group symmetry for every $p$ eigenvectors with equal eigenvalue, properly handling such symmetries is crucial for the stability and generalizability of Laplacian eigenvector augmented GNNs. However, using a naive $O(p)$-group invariant encoder for each $p$-dimensional eigenspace may not keep the full expressivity in the Laplacian eigenvectors. Moreover, computing such invariants inevitably entails a hard split of Laplacian eigenvalues according to their numerical identity, which suffers from great instability when the graph structure has small perturbations. In this paper, we propose a novel method exploiting Laplacian eigenvectors to generate *stable* and globally *expressive* graph representations. The main difference from previous works is that (i) our method utilizes **learnable** $O(p)$-invariant representations for each Laplacian eigenspace of dimension $p$, which are built upon powerful orthogonal group equivariant neural network layers already well studied in the literature, and that (ii) our method deals with numerically close eigenvalues in a **smooth** fashion, ensuring its better robustness against perturbations. Experiments on various graph learning benchmarks witness the competitive performance of our method, especially its great potential to learn global properties of graphs.","Junru Zhou, Cai Zhou, Xiyuan Wang, Pan Li, Muhan Zhang","~Junru_Zhou1, ~Cai_Zhou2, ~Xiyuan_Wang1, ~Pan_Li2, ~Muhan_Zhang1","graph neural networks, graph Laplacian eigenvectors",learning on graphs and other geometries & topologies,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=KmphHE92wU,https://openreview.net/forum?id=KmphHE92wU,,,
SdUMIXCG2a,Accelerated Methods with Complexity Separation Under Data Similarity for Federated Learning Problems,"Heterogeneity within data distribution is an issue encountered in many modern federated learning tasks. We formalize it as an optimization problem with a computationally heavy composite under data similarity. Using different sets of assumptions, we present several approaches to construct communication-efficient methods. An optimal algorithm is proposed for a convex composite. The constructed theory is validated through a series of experiments on various problems such as classification of CIFAR-10 with ResNet-18.",,,"Convex optimization, Data similarity, Composite optimization, Stochastic optimization",optimization,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=SdUMIXCG2a,https://openreview.net/forum?id=SdUMIXCG2a,,,
j84WR5ORsC,Latent Visual Reasoning,"Multimodal Large Language Models (MLLMs) have achieved notable gains in various tasks by incorporating Chain-of-Thought (CoT) reasoning in language spaces. Recent work extends this direction by leveraging external tools for visual editing, thereby enhancing the visual signal along the reasoning trajectories. Nevertheless, these approaches remain fundamentally constrained: reasoning is still confined to the language space, with visual information treated as static preconditions. We introduce Latent Visual Reasoning (LVR), a new paradigm that enables autoregressive reasoning directly in the visual embedding space. A visual encoder first projects images into visual tokens within a joint semantic space shared with the language model. The language model is then trained to generate latent states that reconstruct key visual tokens critical for answering the query, constituting the process of latent visual reasoning. By interleaving LVR with standard text generation, our model achieves substantial gains on perception-intensive visual question answering tasks. In addition, we adapt the GRPO algorithm to conduct reinforcement learning on latent reasoning, further balancing LVR and textual generation.  We show that LVR substantially improves fine-grained visual understanding and perception, achieving 71.67\% on MMVP compared to 66.67\% with Qwen2.5-VL. Code base and model weights will be released later.",,,"multimodal large language models, vision Language Models, multimodal reasoning","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=j84WR5ORsC,https://openreview.net/forum?id=j84WR5ORsC,,,
K9j6iggdGX,A$^2$SG: Adaptive and Asymmetric Surrogate Gradients for Training Deep Spiking Neural Network,"Training deep spiking neural networks (SNNs) remains challenging due to sharp loss landscapes and temporal inconsistency caused by surrogate gradients.
To address these challenges, we propose a unified framework: adaptive and asymmetric surrogate gradients ($\textit{A$^2$SG}$).
The adaptive gradients adjust an effective window for spatio-temporal adaptation, reducing spatial gradient variation and maintaining directional consistency of gradients over time.
The asymmetric gradients reflect neuronal dynamics by assigning larger gradients to neurons with higher membrane potentials, and we prove that they yield lower variation than symmetric surrogates.
Our analysis further establishes a direct connection between local gradient variation and the curvature of the loss landscape, providing a principled explanation for how $\textit{A$^2$SG}$ promotes convergence to flatter minima and improves generalization.
We conduct extensive experiments on diverse models, including CNN-based and Transformer-based SNNs, across various tasks such as image classification using both static and neuromorphic datasets, as well as segmentation.
The results demonstrate that $\textit{A$^2$SG}$ consistently improves accuracy and energy efficiency, establishing it as a general and reliable solution for training deep SNNs.",,,"deep spiking neural networks, surrogate gradients, adaptive, asymmetric","unsupervised, self-supervised, semi-supervised, and supervised representation learning",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=K9j6iggdGX,https://openreview.net/forum?id=K9j6iggdGX,,,
QiUitwJDKI,InnateCoder: Learning Programmatic Options with Foundation Models,"Outside of transfer learning settings, reinforcement learning agents start their learning process from a clean slate. As a result, such agents have to go through a slow process to learn even the most obvious skills required to solve a problem. In this paper, we present InnateCoder, a system that leverages human knowledge encoded in foundation models to provide programmatic policies that encode ""innate skills"" in the form of temporally extended actions, or options. In contrast to existing approaches to learning options, InnateCoder learns them from the general human knowledge encoded in foundation models in a zero-shot setting, and not from the knowledge the agent gains by interacting with the environment. Then, InnateCoder searches for a programmatic policy by combining the programs encoding these options into a larger and more complex program. We hypothesized that InnateCoder's scheme of learning and using options could improve the sampling efficiency of current methods for synthesizing programmatic policies. We evaluated our hypothesis in MicroRTS and Karel the Robot, two challenging domains. Empirical results support our hypothesis, since they show that InnateCoder is more sample efficient than versions of the system that do not use options or learn the options from experience. The policies InnateCoder learns are competitive and often outperform current state-of-the-art agents in both domains.","Rubens O. Moraes, Quazi Asif Sadmine, Hendrik Baier, Levi Lelis","~Rubens_O._Moraes2, ~Quazi_Asif_Sadmine1, ~Hendrik_Baier2, ~Levi_Lelis1","programmatic policies, reinforcement learning, options",reinforcement learning,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=QiUitwJDKI,https://openreview.net/forum?id=QiUitwJDKI,,,
ogKE7LcvW6,"Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escape, and Network Embedding","In this paper, we study the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss using gradient descent (GD). We identify the stationary points of such networks, which significantly slow down loss decrease during training. To capture such points while accounting for the non-differentiability of the loss, the stationary points that we study are directional stationary points, rather than other notions like Clarke stationary points. We show that, if a stationary point does not contain ""escape neurons"", which are defined with first-order conditions, it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the *saddle-to-saddle* training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks: By precluding the saddle escape types that previous works did not rule out, we advance one step closer to a complete picture of the entire dynamics. Moreover, we are also able to fully discuss how network embedding, which is to instantiate a narrower network with a wider network, reshapes the stationary points.","Frank Zhengqing Wu, Berfin Simsek, François Gaston Ged","~Frank_Zhengqing_Wu1, ~Berfin_Simsek1, ~François_Gaston_Ged1","loss landscape, non-smooth, training dynamics, network embedding, over-parameterization, neural network",optimization,ICLR 2025 Poster,2025,https://openreview.net/pdf?id=ogKE7LcvW6,https://openreview.net/forum?id=ogKE7LcvW6,,,
Fty0wTcemV,DELIFT: Data Efficient Language model Instruction Fine-Tuning,"Fine-tuning large language models (LLMs) is crucial for task specialization but often becomes resource-intensive due to redundant or uninformative data. Existing data selection methods typically rely either on computationally expensive gradient-based metrics or static embeddings that fail to adapt dynamically to the model’s evolving state, thus limiting their practical effectiveness. To address this,
we propose DELIFT (Data Efficient Language model Instruction Fine-Tuning), leveraging a novel, computationally efficient utility metric inspired by In-Context Learning (ICL). Our ICL-based metric measures the informational value of each data sample by quantifying its effectiveness as an in-context example in improving model predictions for other samples, reflecting its actual contribution relative to the model’s current state. Integrated with tailored submodular optimization methods, DELIFT systematically selects diverse, informative subsets optimized specifically for each fine-tuning stage: instruction tuning, task-specific adaptation, and continual fine-tuning. Experimental results across multiple datasets and model scales show DELIFT reduces fine-tuning data requirements by up to 70% without compromising performance, consistently outperforming existing methods by up to 26% in effectiveness and efficiency.","Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina Danilevsky","~Ishika_Agarwal1, ~Krishnateja_Killamsetty1, ~Lucian_Popa1, ~Marina_Danilevsky1",Data Efficient Instruction Fine-Tuning;  Data Subset Selection; Submodular Functions,"unsupervised, self-supervised, semi-supervised, and supervised representation learning",ICLR 2025 Poster,2025,https://openreview.net/pdf?id=Fty0wTcemV,https://openreview.net/forum?id=Fty0wTcemV,,,
e5r5j8SiSS,E-DDPG: Dual-Objective Deep Deterministic Policy Gradient for MRI Acceleration and Disease Classification,"Long acquisition times remain a major challenge in clinical MRI, where a fundamental trade-off exists between the acceleration achieved through undersampling and the diagnostic utility of the reconstructed images. We cast the problem of acquiring MRI data within a fixed time budget as a discrete reinforcement learning (RL) task and propose an algorithm based on Deep Deterministic Policy Gradient, referred to as E-DDPG. E-DDPG jointly optimizes sampling patterns, image reconstruction quality, and diagnostic accuracy. We introduces three key innovations: (1) a composite reward function that simultaneously encourages cross-entropy reduction, structural similarity improvement, and decrease in predictive entropy; (2) a percentile-based replay buffer that diversifies learning by equally sampling low- and high-value transitions; and (3) integration of the Straight-Through Gumbel-Softmax mechanism to preserve end-to-end differentiability while enabling discrete action selection. We evaluate E-DDPG against state-of-the-art RL-based methods and ablation variants on the fastMRI/fastMRI+ knee datasets at acceleration factors of 4X, 8X, and 10X, demonstrating its superior performance and validating the effectiveness of each proposed component.",,,"Magnetic Resonance Imaging, Reinforcement Learning, Disease Diagnosis, Sampling Strategy",reinforcement learning,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=e5r5j8SiSS,https://openreview.net/forum?id=e5r5j8SiSS,,,
xrwPLtSCM3,ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning,"Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model by leveraging unlabeled data and limited labeled data across clients in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients (external heterogeneity) and within clients (internal heterogeneity). Most FSSL methods typically design fixed or dynamic weight aggregation strategy on the server (for external) or filter out low-confidence unlabeled samples directly by an empirical threshold to reduce mistakes in local client (for internal). But, the former is hard to precisely fit the real global category distribution due to external heterogeneity, and the latter results in fewer training participation of available samples in FL. To address these issues, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. \emph{I.e.}, we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy to better fit the category distribution across clients; for internal, we include the discarded samples together with other samples into training based upon a positive-negative proxy pool {without compromising wrong pseudo-labels.} Insight experiments \& theoretical analysis show that ProxyFL significantly boost the FSSL performance and convergence.",,,Federated Semi-Supervised Learning; Federated Learning,"other topics in machine learning (i.e., none of the above)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=xrwPLtSCM3,https://openreview.net/forum?id=xrwPLtSCM3,,,
HklCmaVtPS,UW-NET: AN INCEPTION-ATTENTION NETWORK FOR UNDERWATER IMAGE CLASSIFICATION,"The classification of images taken in special imaging environments except air is the first challenge in extending the applications of deep learning. We report on an UW-Net (Underwater Network), a new convolutional neural network (CNN) based network for underwater image classification. In this model, we simulate the visual correlation of background attention with image understanding for special environments, such as fog and underwater by constructing an inception-attention (I-A) module. The experimental results demonstrate that the proposed UW-Net achieves an accuracy of 99.3% on underwater image classification, which is significantly better than other image classification networks, such as AlexNet, InceptionV3, ResNet and Se-ResNet. Moreover, we demonstrate the proposed IA module can be used to boost the performance of the existing object recognition networks. By substituting the inception module with the I-A module, the Inception-ResnetV2 network achieves a 10.7% top1 error rate and a 0% top5 error rate on the subset of ILSVRC-2012, which further illustrates the function of the background attention in the image classifications.","Miao Yang and Ke Hu, Chongyi Li, Zhiqiang Wei",,"Underwater image, Convolutional neural network, Image classification, Inception module, Attention module",,ICLR.cc/2020/Conference,2020,https://openreview.net/pdf?id=HklCmaVtPS,https://openreview.net/forum?id=HklCmaVtPS,"lemonmiao@gmial.com, kexisibest@outlook.com, lichongyi@tju.edu.cn, weizhiqiang@ouc.edu.cn",1570000000000.0,
kcmK2utDhu,MV-Adapter: Multi-view Consistent Image Generation Made Easy,"Generating multi-view images of an object has important applications in content creation and perception. Existing methods achieved this by making invasive changes to pre-trained text-to-image (T2I) models and performing full-parameter training, leading to three main limitations: (1) High computational costs, especially for high-resolution outputs; (2) Incompatibility with derivatives and extensions of the base model, such as personalized models, distilled few-step models, and plugins like ControlNets; (3) Limited versatility, as they primarily serve a single purpose and cannot handle diverse conditioning signals such as text, images, and geometry. In this paper, we present MV-Adapter to address all the above limitations. MV-Adapter is designed to be a plug-and-play module working on top of pre-trained T2I models. This enables efficient training for high-resolution synthesis while maintaining full compatibility with all kinds of derivatives of the base T2I model. MV-Adapter provides a unified implementation for generating multi-view images from various conditions, facilitating applications such as text- and image-based 3D generation and texturing. We demonstrate that MV-Adapter sets a new quality standard for multi-view image generation, and opens up new possibilities due to its adaptability and versatility.","Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, Lu Sheng","~Zehuan_Huang1, ~Yuan-Chen_Guo1, ~Haoran_Wang17, ~Ran_Yi1, ~Lizhuang_Ma1, ~Yan-Pei_Cao1, ~Lu_Sheng1","Multi-view Image Generation, 3D Generation, Diffusion Models",generative models,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=kcmK2utDhu,https://openreview.net/forum?id=kcmK2utDhu,,,
WMInu1V9av,AURA: Visually Interpretable Affective Understanding via Robust Archetypes,"Text-driven vision--language methods (e.g., CLIP variants) face three persistent hurdles in affective computing: (i) limited support for continuous regression (e.g., Valence--Arousal), (ii) brittle reliance on language prompts, and (iii) the absence of a unified paradigm across expression classification, action-unit detection, and affective regression. We introduce AURA, a prompt-free framework that operates directly in a frozen CLIP visual space via visual archetypes. AURA comprises two components: (1) self-organized archetype discovery, which adaptively allocates the number of archetypes per affective state, assigning denser archetype sets to complex or ambiguous states for fine-grained interpretability, and (2) archetype contextualization, which models interactions among the most relevant archetypes and semantic tokens to enhance structural consistency while suppressing redundancy. Inference reduces to cosine matching between projected features and fixed archetypes. Across six datasets, AURA consistently surpasses prior state-of-the-art while remaining highly efficient. Overall, AURA unifies classification, detection, and regression under a single visual-archetype paradigm, delivering strong accuracy, cognitively aligned interpretability, and excellent training/inference efficiency.",,,"valence-arousal, facial expressions, action units","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=WMInu1V9av,https://openreview.net/forum?id=WMInu1V9av,,,
HyeJmlrFvH,Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization,"As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.","Ali Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan Alistarh, Daniel M. Roy",,,,ICLR.cc/2020/Conference,2020,https://openreview.net/pdf?id=HyeJmlrFvH,https://openreview.net/forum?id=HyeJmlrFvH,"alir@vectorinstitute.ai, faghri@cs.toronto.edu, droy@utstat.toronto.edu, dan.alistarh@ist.ac.at, markovilya197@gmail.com, vitalii.aksenov@ist.ac.at",1570000000000.0,
5Y8PMEeAkv,ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models,"As the capabilities of Vision-Language Models (VLMs) advance, they can process increasingly large inputs, which, unlike in LLMs, generates significant visual token redundancy and leads to prohibitive inference costs. While many methods aim to reduce these costs by pruning visual tokens, existing approaches, whether based on attention or diversity, typically neglect the guidance of the text prompt and thus fail to prioritize task relevance. In this work, we propose a novel, zero-shot method that reframes the problem by introducing a prompt-aware perspective, explicitly modeling visual token pruning as a balance between task relevance and information diversity. Our hierarchical approach first selects a core set of task-relevant visual tokens and then supplements them with diversity tokens to preserve broader context. Experiments across multiple models and benchmarks show that our method achieves performance that matches or surpasses the state-of-the-art with only minimal accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these gains are accompanied by significant reductions in GPU memory footprint and inference latency.",,,"VLM, Prompt-Aware, Zero-Shot, Visual token pruning","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=5Y8PMEeAkv,https://openreview.net/forum?id=5Y8PMEeAkv,,,
90UhF7e8jo,Goal Achievement Guided Exploration: Mitigating Premature Convergence in Reinforcement Learning,"Premature convergence to suboptimal policies remains a significant challenge in reinforcement learning (RL), particularly in tasks with sparse rewards or non-convex reward landscapes. Existing work usually utilizes reward shaping, such as curiosity-based internal rewards, to encourage exploring promising spaces. However, this may inadvertently introduce new local optima and impair the optimization for the actual target reward. To address this issue, we propose Goal Achievement Guided Exploration (GAGE), a novel approach that incorporates an agent's goal achievement as a dynamic criterion for balancing exploration and exploitation. GAGE adaptively adjusts the exploitation level based on the agent's current performance relative to an estimated optimal performance, thereby mitigating premature convergence. Extensive evaluations demonstrate that GAGE substantially improves learning outcomes across various challenging tasks by adapting convergence based on task success. Applicable to both continuous and discrete tasks, GAGE seamlessly integrates into existing RL frameworks, highlighting its potential as a versatile tool for enhancing exploration strategies in RL.","Shengchao Yan, Baohe Zhang, Joschka Boedecker, Wolfram Burgard","~Shengchao_Yan1, ~Baohe_Zhang1, ~Joschka_Boedecker1, ~Wolfram_Burgard3","reinforcement learning, exploration, deep reinforcement learning",reinforcement learning,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=90UhF7e8jo,https://openreview.net/forum?id=90UhF7e8jo,,,
3RhuF8foyPW,Single-shot General Hyper-parameter Optimization for Federated Learning,"We address the problem of hyper-parameter optimization (HPO) for federated learning (FL-HPO). We introduce Federated Loss SuRface Aggregation (FLoRA), a general FL-HPO solution framework that can address use cases of tabular data and any Machine Learning (ML) model including gradient boosting training algorithms, SVMs, neural networks, among others and thereby further expands the scope of FL-HPO. FLoRA enables single-shot FL-HPO: identifying a single set of good hyper-parameters that are subsequently used in a single FL training. Thus, it enables FL-HPO solutions with minimal additional communication overhead compared to FL training without HPO. Utilizing standard smoothness assumptions, we theoretically characterize the optimality gap of FLoRA for any convex and non-convex loss functions, which explicitly accounts for the heterogeneous nature of the parties' local data distributions, a dominant characteristic of FL systems. Our empirical evaluation of FLoRA for multiple FL algorithms on seven OpenML datasets demonstrates significant model accuracy improvements over the baselines, and robustness to increasing number of parties involved in FL-HPO training.","Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, Heiko Ludwig",,"Federated Learning, Hyperparameter Optimization, Optimality Gap Analysis",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=3RhuF8foyPW,https://openreview.net/forum?id=3RhuF8foyPW,"~Yi_Zhou13, ~Parikshit_Ram1, ~Theodoros_Salonidis1, ~Nathalie_Baracaldo1, ~Horst_Samulowitz1, ~Heiko_Ludwig1",1660000000000.0,
601vEMLqie,DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion,"Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions.",,,"Diffusion, Text to Image, High Resolution",generative models,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=601vEMLqie,https://openreview.net/forum?id=601vEMLqie,,,
7qXmJbjbl8,Attribute-Centric Representation Learning for Interpretable Crime Scene Analysis in Video Anomaly Detection,"Automatic crime scene analysis is an important application area for representation learning in Video Anomaly Detection (VAD). Effective interpretation of anomalous events requires models to learn rich, disentangled representations that capture fine-grained, crime-relevant attributes. However, widely used VAD datasets (e.g., UCA, CUVA) primarily offer coarse event-level labels and they lack attribute-level supervision often needed for modeling crime-specific behaviors. To bridge this gap, we propose an attribute-centric learning framework that explicitly conditions video representations on crime-causing attributes. We extend the UCA dataset with over 1.5M new attribute-centric annotations generated using carefully designed prompts and LLMs. These annotations enable supervised fine-tuning of a curated CLIP-based model, leading to more discriminative, attribute-aware video representations, and precise event captions. An LLM-based summarizer then distills these captions into context-rich explanations, facilitating interpretable scene understanding. Our approach answers three core questions in crime scene analysis: \textbf{What? When? How?} Extensive experiments show that the proposed representation learning framework yields significant improvements ($\approx 20\%\uparrow$) in attribute-centric crime classification accuracy and ($\approx 6.4\%\uparrow$) according to MMEval scores over the baselines. We further analyze and mitigate biases in MMEval to ensure robustness and fair evaluation. These results highlight the importance of attribute-conditioned representation learning for interpretable and reliable VAD.",,,"Crime Scene Analysis, Video Anomaly Detection, Explainable AI, Visual Language Reasoning",interpretability and explainable AI,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=7qXmJbjbl8,https://openreview.net/forum?id=7qXmJbjbl8,,,
mEBSeSk49H,On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond,"This paper aims to clearly distinguish between Stochastic Gradient Descent with Momentum (SGDM) and Adam in terms of their convergence rates. We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in stochastic setting, Adam's convergence rate upper bound matches the lower bounds of stochastic first-order optimizers, considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate. These insights distinctly differentiate Adam and SGDM regarding their convergence rates. Additionally, by introducing a novel stopping-time based technique, we further prove that if we consider the minimum gradient norm during iterations, the corresponding convergence rate can match the lower bounds across all problem hyperparameters. The technique can also help proving that Adam with a specific hyperparameter scheduler is parameter-agnostic, which hence can be of independent interest.","Bohan Wang, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-Ming Ma, Wei Chen","~Bohan_Wang1, ~Huishuai_Zhang3, ~Qi_Meng1, ~Ruoyu_Sun1, ~Zhi-Ming_Ma1, ~Wei_Chen1","Adam, Convergence, Separability, Non-uniform Smoothness",learning theory,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=mEBSeSk49H,https://openreview.net/forum?id=mEBSeSk49H,,,
qO1cJBh5BX,Probabilistic DiffusionNet: A geometry informed probabilistic generative surrogate for PDEs,"We propose a probabilistic generative extension of the DiffusionNet architecture, widely used for surface learning tasks, by introducing latent random variables derived from a stochastic reformulation of the underlying diffusion process. The resulting probabilistic model can be used as a resolution-invariant and uncertainty-aware surrogate for the trace solution map of PDEs whose boundary conditions are determined by surface geometry. Such a surrogate can expedite and inform typical engineering design and optimisation processes that require computationally burdensome computational fluid dynamics (CFD) analysis pipelines. We demonstrate that the proposed architecture produces superior uncertainty quantification (UQ) performance on standard CFD datasets without sacrificing predictive accuracy, while enjoying lower computational cost compared to other prevalent geometry-informed PDE surrogates endowed with UQ capabilities.",,,"SPDE, DiffusionNet, PDE, CFD, UQ, Variational Inference, VAE, Generative Models, Stochastic Process, Gaussian Process, Neural Operators","probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=qO1cJBh5BX,https://openreview.net/forum?id=qO1cJBh5BX,,,
e7AUJpP8bV,PAD: Personalized Alignment of LLMs at Decoding-time,"Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods. In response, this paper presents Personalized Alignment at Decoding-time (PAD), a novel framework designed to align LLM outputs with diverse personalized preferences during the inference phase, eliminating the need for additional training. By introducing a unique personalized reward modeling strategy, this framework decouples the text generation process from personalized preferences, facilitating the generation of generalizable token-level personalized rewards. The PAD algorithm leverages these rewards to guide the decoding process, dynamically tailoring the base model’s predictions to personalized preferences. Extensive experimental results demonstrate that PAD not only outperforms existing training-based alignment methods in terms of aligning with diverse preferences but also shows significant generalizability to preferences unseen during training and scalability across different base models. This work advances the capability of LLMs to meet user needs in real-time applications, presenting a substantial step forward in personalized LLM alignment.","Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, Zuozhu Liu","~Ruizhe_Chen1, ~Xiaotian_Zhang1, ~Meng_Luo2, ~Wenhao_Chai1, ~Zuozhu_Liu1","Alignment, personalization","alignment, fairness, safety, privacy, and societal considerations",ICLR 2025 Poster,2025,https://openreview.net/pdf?id=e7AUJpP8bV,https://openreview.net/forum?id=e7AUJpP8bV,,,
VB2WkqvFwF,The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets,"We study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure. We focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. Our main observations are: (i) The power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated normally distributed data compared to real-world data, (ii) this scaling behavior can be completely modeled by generating gaussian data with long range correlations, (iii) both generated and real-world datasets lie in the same universality class from the RMT perspective, as chaotic rather than integrable systems, (iv) the expected RMT statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conventionally used for real-world training, and can be related to the number of samples required to approximate the population power-law scaling behavior, (v) the Shannon entropy is correlated with local RMT structure and eigenvalues scaling, and substantially smaller in strongly correlated datasets compared to uncorrelated synthetic data, and requires fewer samples to reach the distribution entropy. These findings show that with sufficient sample size, the Gram matrix of natural image datasets can be well approximated by a Wishart random matrix with a simple covariance structure, opening the door to rigorous studies of neural network dynamics and generalization which rely on the data Gram matrix.","Noam Itzhak Levi, Yaron Oz","~Noam_Itzhak_Levi1, ~Yaron_Oz1","Random Matrix Theory, Data Structure, Universality, Random Feature Models, Empirical Data Estimation, Neural Scaling Laws","representation learning for computer vision, audio, language, and other modalities",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=VB2WkqvFwF,https://openreview.net/forum?id=VB2WkqvFwF,,,
q8fTgw8e5E,Translation Heads: Unveiling Attention's Role in LLM Multilingual Translation,"Recently, large language models (LLMs) have made remarkable progress, with multilingual capability emerging as a core foundational strengths. However, the internal mechanisms by which these models perform translation remain incompletely understood. In this paper, we elucidate the relationship between the attention mechanism in LLMs and their translation abilities. We find that certain attention heads, which we term translation heads, are specifically responsible for mapping tokens from the source language to the target language during inference. Through a systematic investigation across various models, we confirm that these translation heads exhibit several key characteristics: (1) Universality: They are present in all LLMs we studied. (2) Sparsity: They constitute only a small fraction of all attention heads. (3) Consistency: The set of translation heads activated by the model shows strong consistency across different language pairs. (4) Causality: Interventionally removing these heads leads to a sharp decline in the model's translation performance, while randomly removing non-translation heads has little impact on translation ability. (5) Functional Specificity: Ablating translation heads disproportionately harms translation but has a varied impact on other multilingual tasks. We also traced the formation of translation heads during pre-training, revealing an evolutionary path of rapid proliferation, stabilization, and eventual pruning. Furthermore we leverage these translation heads to filter multilingual training data, and our experiments show that these datas significantly enhance translation capabilities of the models.",,,"LLM, Multilinguistic, Interpretability","foundation or frontier models, including LLMs",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=q8fTgw8e5E,https://openreview.net/forum?id=q8fTgw8e5E,,,
tyyNcEVrklJ,Decentralized Policy Optimization,"The study of decentralized learning or independent learning in cooperative multi-agent reinforcement learning has a history of decades. Recently empirical studies show that independent PPO (IPPO) can obtain good performance, close to or even better than the methods of centralized training with decentralized execution, in several benchmarks. However, decentralized actor-critic with convergence guarantee is still open. In this paper, we propose decentralized policy optimization (DPO), a decentralized actor-critic algorithm with monotonic improvement and convergence guarantee. We derive a novel decentralized surrogate for policy optimization such that the monotonic improvement of joint policy can be guaranteed by each agent independently optimizing the surrogate. In practice, this decentralized surrogate can be realized by two adaptive coefficients for policy optimization at each agent. Empirically, we compare DPO with IPPO in a variety of cooperative multi-agent tasks, covering discrete and continuous action spaces, and fully and partially observable environments. The results show DPO outperforms IPPO in most tasks, which can be the evidence for our theoretical results.","Kefan Su, Zongqing Lu",,multi-agent reinforcement learning,,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=tyyNcEVrklJ,https://openreview.net/forum?id=tyyNcEVrklJ,"~Kefan_Su1, ~Zongqing_Lu2",1660000000000.0,
8xuFD1yCoH,TuneUp: A Training Strategy for Improving Generalization of Graph Neural Networks,"Despite many advances in Graph Neural Networks (GNNs), their training strategies simply focus on minimizing a loss over nodes in a graph. However, such simplistic training strategies may be sub-optimal as they neglect that certain nodes are much harder to make accurate predictions on than others. Here we present TuneUp, a curriculum learning strategy for better training GNNs. Crucially, TuneUp trains a GNN in two stages. The first stage aims to produce a strong base GNN. Such base GNNs tend to perform well on head nodes (nodes with large degrees) but less so on tail nodes (nodes with small degrees). So, the second stage of TuneUp specifically focuses on improving prediction on tail nodes. Concretely, TuneUp synthesizes many additional supervised tail node data by dropping edges from head nodes and reusing the supervision on the original head nodes. TuneUp then minimizes the loss over the synthetic tail nodes to finetune the base GNN. TuneUp is a general training strategy that can be used with any GNN architecture and any loss, making TuneUp applicable to a wide range of prediction tasks. Extensive evaluation of TuneUp on two GNN architectures, three types of prediction tasks, and both inductive and transductive settings shows that TuneUp significantly improves the performance of the base GNN on tail nodes, while often even improving the performance on head nodes, which together leads up to 58.5% relative improvement in GNN predictive performance. Moreover, TuneUp significantly outperforms its variants without the two-stage curriculum learning, existing graph data augmentation techniques, as well as other specialized methods for tail nodes.","Weihua Hu, Kaidi Cao, Kexin Huang, Edward W Huang, Karthik Subbian, Jure Leskovec",,"Graph Neural Networks, Curriculum learning, Tail nodes",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=8xuFD1yCoH,https://openreview.net/forum?id=8xuFD1yCoH,"~Weihua_Hu1, ~Kaidi_Cao1, ~Kexin_Huang1, ~Edward_W_Huang1, ~Karthik_Subbian1, ~Jure_Leskovec1",1660000000000.0,
7sWLxZBLPO5,Multiple Modes for Continual Learning,"Adapting model parameters to incoming streams of data is a crucial factor to deep learning scalability. Interestingly, prior continual learning strategies in online settings inadvertently anchor their updated parameters to a local parameter subspace to remember old tasks, else drift away from the subspace and forget. From this observation, we formulate a trade-off between constructing multiple parameter modes and allocating tasks per mode. Mode-Optimized Task Allocation (MOTA), our contributed adaptation strategy, trains multiple modes in parallel, then optimizes task allocation per mode. We empirically demonstrate improvements over baseline continual learning strategies and across varying distribution shifts, namely sub-population, domain, and task shift.","Siddhartha Datta, Nigel Shadbolt",,,,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=7sWLxZBLPO5,https://openreview.net/forum?id=7sWLxZBLPO5,"~Siddhartha_Datta1, ~Nigel_Shadbolt1",1660000000000.0,
fBlHaSGKNg,Unleashing the Power of Annotation: Enhancing Semi-Supervised Learning through Unsupervised Sample Selection,"With large volumes of unlabeled data and limited annotation budgets, Semi-Supervised Learning (SSL) has become a preferred approach in many deep learning tasks. However, most previous studies have primarily focused on  utilizing labeled and unlabeled data for model training to improve performance, while the efficient selection of samples for annotation under budgetary constraints has often been overlooked.To fill this gap, we propose an efficient sample selection methodology named Unleashing the Power of Annotation (UPA). By adopting a modified Frank-Wolfe algorithm to minimizing a novel criterion $\alpha$-Maximum Mean Discrepancy ($\alpha$-MMD), UPA selects a representative and diverse subset for annotation from the unlabeled data. Furthermore, we demonstrate that minimizing $\alpha$-MMD enhances the generalization ability of low-budget learning. Experiments show that UPA consistently improves the performance of several popular SSL methods, surpassing various prevailing Active Learning (AL) and Semi-Supervised Active Learning (SSAL) methods even under constrained annotation budgets.","Qian Shao, Jiangrui Kang, Qiyuan Chen, Hongxia Xu, Yiwen Cao, JIAJUAN LIANG, Jian Wu","~Qian_Shao2, ~Jiangrui_Kang1, ~Qiyuan_Chen1, ~Hongxia_Xu1, ~Yiwen_Cao2, ~JIAJUAN_LIANG1, ~Jian_Wu6","semi-supervised learning, unsupervised sample selection","unsupervised, self-supervised, semi-supervised, and supervised representation learning",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=fBlHaSGKNg,https://openreview.net/forum?id=fBlHaSGKNg,,,
DjeQ39QoLQ,Robustifying State-space Models for Long Sequences via Approximate Diagonalization,"State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable ``perturb-then-diagonalize'' (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.","Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson","~Annan_Yu1, ~Arnur_Nigmetov1, ~Dmitriy_Morozov1, ~Michael_W._Mahoney1, ~N._Benjamin_Erichson1","state-space models, sequence models, Long-Range Arena, recurrent neural networks","general machine learning (i.e., none of the above)",ICLR 2024 spotlight,2024,https://openreview.net/pdf?id=DjeQ39QoLQ,https://openreview.net/forum?id=DjeQ39QoLQ,,,
QDmoLEJifR,Behavioral Embeddings of Programs: A Quasi-Dynamic Approach for Optimization Prediction,"Learning effective numerical representations, or embeddings, of programs is a fundamental prerequisite for applying machine learning to automate and enhance compiler optimization. Prevailing paradigms, however, present a dilemma. Static representations, derived from source code or intermediate representation (IR), are efficient and deterministic but offer limited insight into how a program will behave or evolve under complex code transformations. Conversely, dynamic representations, which rely on runtime profiling, provide profound insights into performance bottlenecks but are often impractical for large-scale tasks due to prohibitive overhead and inherent non-determinism. This paper transcends this trade-off by proposing a novel quasi-dynamic framework for program representation. The core insight is to model a program's optimization sensitivity. We introduce the Program Behavior Spectrum, a new representation generated by probing a program's IR with a diverse set of optimization sequences and quantifying the resulting changes in its static features. To effectively encode this high-dimensional, continuous spectrum, we pioneer a compositional learning approach. Product Quantization is employed to discretize the continuous reaction vectors into structured, compositional sub-words. Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to learn the deep contextual grammar of these behavioral codes. Comprehensive experiments on two representative compiler optimization tasks---Best Pass Prediction and -Oz Benefit Prediction---demonstrate that our method outperforms state-of-the-art static baselines. Our code is publicly available at https://anonymous.4open.science/r/PREP-311F/.",,,"Program Representation, Compiler Optimization, Behavioral Embedding","unsupervised, self-supervised, semi-supervised, and supervised representation learning",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=QDmoLEJifR,https://openreview.net/forum?id=QDmoLEJifR,,,
dAn82lpLx4,The Agent's Marathon: Probing the Limits of Endurance in Long-Horizon Tasks,"Large Language Model (LLM) agents, augmented with diverse tools, have shown impressive progress in domains such as scientific discovery and enterprise automation. Yet they remain brittle in long-horizon tasks that require extended sequences of interactions, where performance often deteriorates rapidly. Existing benchmarks provide only partial coverage of this challenge: manual or crowdsourced tasks are too short, tool-use benchmarks emphasize breadth over depth, and web-based evaluations rely on emergent rather than controllable complexity. To fill this gap, we introduce TaskWeaver, a rule-based, controllable platform for generating benchmark tasks with precisely adjustable difficulty and horizon length. At its core, TaskWeaver abstracts all tool use as file-read operations. This design choice removes superficial API complexities, allowing us to directly probe an agent’s core ability to reason and integrate intermediate results over long, dependent sequences. We instantiate the framework across three domains: document understanding and navigation, multi-modal information integration, and executable code analysis. Each domain probes a complementary aspect of agentic reasoning, and together they form a unified benchmark, LORE (Long-horizon Reasoning Evaluation). Empirical results show that even for the strongest models we tested, performance degrades significantly as task length and per-step complexity increase. Specifically, their accuracy approaches zero on tasks exceeding 120 steps, and on more challenging variants, performance collapses in fewer than 15 steps. These findings highlight long-horizon robustness as a central open challenge for future agent development.",,,"natural language model, agent, benchmark",datasets and benchmarks,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=dAn82lpLx4,https://openreview.net/forum?id=dAn82lpLx4,,,
3X3LuwzZrl,Multi-Label Node Classification with Label Influence Propagation,"Graphs are a complex and versatile data structure used across various domains, with possibly multi-label nodes playing a particularly crucial role. 
Examples include proteins in PPI networks with multiple functions and users in social or e-commerce networks exhibiting diverse interests. 
Tackling multi-label node classification (MLNC) on graphs has led to the development of various approaches. Some methods leverage graph neural networks (GNNs) to exploit label co-occurrence correlations, while others incorporate label embeddings to capture label proximity. However, these approaches fail to account for the intricate influences between labels in non-Euclidean graph data.
To address this issue, we decompose the message passing process in GNNs into two operations: propagation and transformation. 
We then conduct a comprehensive analysis and quantification of the influence correlations between labels in each operation. 
Building on these insights, we propose a novel model, Label Influence Propagation (LIP). 
Specifically, we construct a label influence graph based on the integrated label correlations. 
Then, we propagate high-order influences through this graph, dynamically adjusting the learning process by amplifying labels with positive contributions and mitigating those with negative influence.
Finally, our framework is evaluated on comprehensive benchmark datasets, consistently outperforming SOTA methods across various settings, demonstrating its effectiveness on MLNC tasks.","Yifei Sun, Zemin Liu, Bryan Hooi, Yang Yang, Rizal Fathony, Jia Chen, Bingsheng He","~Yifei_Sun1, ~Zemin_Liu1, ~Bryan_Hooi1, ~Yang_Yang35, ~Rizal_Fathony1, ~Jia_Chen2, ~Bingsheng_He1","graph neural networks, multi-label, node classification",learning on graphs and other geometries & topologies,ICLR 2025 Poster,2025,https://openreview.net/pdf?id=3X3LuwzZrl,https://openreview.net/forum?id=3X3LuwzZrl,,,
FjifPJV2Ol,SOLVING SCHRODINGER BRIDGE PROBLEM VIA STOCHASTIC ACTION MINIMIZATION,"The Schrodinger bridge problem is a classical entropy-regularized optimal transport problem that seeks to find optimal diffusion trajectories that transform one probability distribution into another. Although mathematical theory has reached
a mature stage, the ongoing research in algorithmic advancements remains a dynamic field, driven by recent innovations in diffusion models. We introduce stochastic Lagrangian and stochastic action as viable alter-
native for serving as a direct loss function. We demonstrate the feasibility of incorporating all the vital physical constraints necessary to solve the problem directly into the Lagrangian,  providing an intuitive grasp of the loss function and streamlining the training process.","Mykola Bordyuh, Daniel Wong, Robert Moccia, Djork-Arné Clevert","~Mykola_Bordyuh1, daniel.wong@pfizer.com, robert.moccia@pfizer.com, ~Djork-Arné_Clevert2","Schrodinger bridge, optimal transport, single-cell, trajectories",generative models,Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=FjifPJV2Ol,https://openreview.net/forum?id=FjifPJV2Ol,,,
Bp0HBaMNRl,Differentiable Causal Discovery for Latent Hierarchical Causal Models,"Discovering causal structures with latent variables from observational data is a fundamental challenge in causal discovery. Existing methods often rely on constraint-based, iterative discrete searches, limiting their scalability for large numbers of variables. Moreover, these methods frequently assume linearity or invertibility, restricting their applicability to real-world scenarios. We present new theoretical results on the identifiability of non-linear latent hierarchical causal models, relaxing previous assumptions in the literature about the deterministic nature of latent variables and exogenous noise. Building on these insights, we develop a novel differentiable causal discovery algorithm that efficiently estimates the structure of such models. To the best of our knowledge, this is the first work to propose a differentiable causal discovery method for non-linear latent hierarchical models. Our approach outperforms existing methods in both accuracy and scalability. Furthermore, we demonstrate its practical utility by learning interpretable hierarchical latent structures from high-dimensional image data and demonstrate its effectiveness on downstream tasks such as transfer learning.","Parjanya Prajakta Prashant, Ignavier Ng, Kun Zhang, Biwei Huang","~Parjanya_Prajakta_Prashant1, ~Ignavier_Ng1, ~Kun_Zhang1, ~Biwei_Huang1","Differentiable causal discovery, causal representation learning, latent variable models, causal structure learning, causal identifiability",causal reasoning,ICLR 2025 Poster,2025,https://openreview.net/pdf?id=Bp0HBaMNRl,https://openreview.net/forum?id=Bp0HBaMNRl,,,
AEszIYnVos,NECromancer: Breathing Life into Skeletons via BVH Animation,"Motion tokenization is fundamental to the development of generalizable motion models, yet existing approaches remain restricted to species-specific skeletons, such as humans, thereby limiting their applicability across diverse morphologies. We present NECromancer (NEC), a universal motion tokenizer designed to operate on arbitrary BVH skeletons. NEC is built upon three core components: (1) an
Ontology-aWare Skeletal Graph EncOder (OwO), which leverages graph neural networks to encode structural priors extracted from BVH files—including joint-name semantics, rest-pose offsets, and skeletal topology—into robust skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT), which compresses motion sequences into a universal, topology–invariant latent representation, thereby decoupling motion dynamics from morphology; and (3) the Unified BVH Universe (UvU), a large-scale dataset that consolidates BVH motions across heterogeneous skeletons (humans, quadrupeds, and other species), enabling systematic training and evaluation under diverse morphologies. Experimental results demonstrate that NEC achieves high-fidelity motion reconstruction with substantial compression, while effectively disentangling motion from skeletal structure. This capability supports a broad range of downstream tasks, including cross-species motion transfer, motion composition, denoising, generation (plug-and-play with any token-based generator; e.g., MoMask) and motion–text retrieval (via an OwO-based CLIP variant). By grounding motion representation in BVH animation while removing species-specific constraints, NEC establishes a principled framework for universal motion analysis and synthesis across varied morphologies.",,,Motion tokenization，Motion Generation，BVH Animation，Skeletal-Invariant Representation，Cross-Species Motion Transfer，Motion Compression,generative models,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=AEszIYnVos,https://openreview.net/forum?id=AEszIYnVos,,,
EO6WtJ0q6G,Do Large Language Models Know What They Are Capable Of?,"We investigate whether large language models (LLMs) can predict whether they will succeed on a given task, and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparable to or worse than non-reasoning LLMs. With in-context experiences of failure, most LLMs only slightly reduce their overconfidence, though in a resource acquisition scenario several LLMs (Claude Sonnet models and GPT-4.5) improve their performance by increasing their risk aversion. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.",,,"LLM Calibration, Decision Making, Overconfidence, In-context learning, LLM Agents, LLM self-knowledge, AI Safety","foundation or frontier models, including LLMs",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=EO6WtJ0q6G,https://openreview.net/forum?id=EO6WtJ0q6G,,,
5XL8c0Vg9k,Infinite-parameter Large Language Model,"In the standard transformer architecture, increasing model parameters leads to linear growth in computational cost and activation memory. To address this issue, we propose a novel Infinite Parameter Large Language Model (IP-LLM) architecture that decouples model size from computational cost and device memory. Existing large language models are all fixed-parameter models, while human knowledge is infinite and expands daily. Finite parameters are inherently limited in their capacity to accommodate this boundless knowledge. Our IP-LLM architecture can potentially accommodate infinite knowledge, resolving this issue and laying the foundation for realizing a truly omniscient and omnipotent artificial general intelligence in the future.",Fei Ding,~Fei_Ding6,lifelong learning,"transfer learning, meta learning, and lifelong learning",Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=5XL8c0Vg9k,https://openreview.net/forum?id=5XL8c0Vg9k,,,
bgyWXX8HCk,Trustless Audits without Revealing Data or Models,"There is an increasing conflict between business incentives to hide models and data as trade secrets, and the societal need for algorithmic transparency. For example, a rightsholder who currently wishes to know whether their copyrighted works have been used during training must convince the model provider to allow a third party to audit the model and data. Finding a mutually agreeable third party is difficult, and the associated costs often make this approach impractical.

In this work, we show that it is possible to simultaneously allow model providers to keep their models and data secret while allowing other parties to trustlessly audit properties of the model and data. We do this by designing a protocol called ZkAudit in which model providers publish cryptographic commitments of datasets and model weights, alongside a zero-knowledge proof (ZKP) certifying that published commitments are derived from training the model. Model providers can then respond to audit requests by privately computing any function F of the dataset (or model) and releasing the output of F alongside another ZKP certifying the correct execution of F. To enable ZkAudit, we develop new methods of computing ZKPs for SGD on modern neural nets for recommender systems and image classification models capable of high accuracies on ImageNet. Empirically, we show it is possible to provide trustless audits of DNNs, including copyright, censorship, and counterfactual audits with little to no loss in accuracy.","Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel Kang","~Suppakit_Waiwitlikhit3, ~Ion_Stoica1, ~Yi_Sun3, ~Tatsunori_Hashimoto1, ~Daniel_Kang1","trustless audit, zero-knowledge proof","societal considerations including fairness, safety, privacy",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=bgyWXX8HCk,https://openreview.net/forum?id=bgyWXX8HCk,,,
CyKVrhNABo,Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences,"Submissions are rising fast, and venues use different rules, data formats, and update times. As a result, signals of progress get split across places, and key moments (rebuttal, discussion, final decision) are easy to miss, making analysis hard. We present Paper Copilot, a system and scalable peer-review archive that pulls data from official sites, OpenReview, and opt-in forms into a single, standardized, versioned record with timestamps. This lets us track trends over time and compare venues, institutions, and countries in a consistent way. Using the archive for ICLR 2024/2025, we see larger score changes after rebuttal for higher-tier papers, reviewer agreement that dips during active discussion and tightens by the end, and in 2025 a sharper, mean-score–driven assignment of tiers with lower decision uncertainty than expected at that scale. We also state simple rules for ethics—clear sourcing and consent, privacy protection, and limits on use for closed venues. Together, we provide a clear, reusable base for tracking AI/ML progress, and, with this data, enable validation, benchmarking, and otherwise hard-to-run studies.",,,"peer review, review dynamic, aiml, community","infrastructure, software libraries, hardware, systems, etc.",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=CyKVrhNABo,https://openreview.net/forum?id=CyKVrhNABo,,,
VGnOJhd5Q1q,Sparse Attention with Learning to Hash,"Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.","Zhiqing Sun, Yiming Yang, Shinjae Yoo",,"Sparse Attention, Transformer, Learning-to-Hash, Natural Language Processing",,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=VGnOJhd5Q1q,https://openreview.net/forum?id=VGnOJhd5Q1q,"~Zhiqing_Sun1, ~Yiming_Yang1, ~Shinjae_Yoo1",1630000000000.0,
EURAfiUpVJ,MARS-SQL: A Multi-Agent Reinforcement Learning Framework for Text-to-SQL,"Translating natural language to SQL remains a significant challenge for complex queries requiring environmental interaction and self-correction. To address this, we introduce MARS-SQL, a novel multi-agent framework that combines principled task decomposition and interactive reinforcement learning (RL). Our system comprises three specialized agents: a Grounding Agent for schema linking, a Generation Agent for query generation, and a Validation Agent for final selection. The core of our framework is the Generator agent, which is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe loop, the agent iteratively generates thoughts, executes SQL actions against a live database, and revises its strategy based on execution feedback, enabling dynamic, stateful reasoning and self-correction. At inference time, we generate multiple interaction trajectories to explore diverse reasoning paths. The Verifier agent, then selects the optimal trajectory by modeling verification as a next-token prediction task and choosing the solution with the highest generation probability. This structured workflow, which pipelines specialized agents and combines interactive RL for generation with generative modeling for verification, proves highly effective for robust and accurate SQL generation. Experiments show that **MARS-SQL** achieves state-of-the-art Execution Accuracy of 77.84\% on the BIRD dev set and 89.75\% on the Spider test set.",,,"Text2SQL, LLM, Reinforcement Learning, Multi-Agent","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=EURAfiUpVJ,https://openreview.net/forum?id=EURAfiUpVJ,,,
gqmELEp3lZ,OpenReviewer: Mitigating Challenges in LLM Reviewing,"Human reviews of research papers are slow and of variable quality. Hence there is increasing interest in using large language models (LLMs) such as GPT to review research papers. This paper develops a proof-of-concept LLM review process that shows LLMs offer consistently high-quality reviews almost instantly. However, many challenges and limitations remain: risk of misuse, inflated review scores, overconfident ratings, skewed score distributions, and limited prompt length. We mitigate these issues without prompt engineering by using LLM watermarking to mark LLM-generated reviews; classifying and detection errors and shortcomings of papers; and using long-context windows that include the review form, entire paper, reviewer guidelines, code of ethics and conduct, area chair guidelies, and previous year statistics; and a blind human evaluation of reviews. We aim to use OpenReviewer to review and revise research papers, improving their quality. This work identifies and addresses drawbacks associated with GPT as a reviewer and enhances the quality of the reviewing process based on a randomized human blind evaluation. Making OpenReviewer available as an open online service that generates reviews will allow the use of scalable human feedback to learn and improve.","Keith Tyser, Jason Lee, Avi Shporer, Madeleine Udell, Dov Te'eni, Iddo Drori","~Keith_Tyser1, ~Jason_Lee6, ~Avi_Shporer1, ~Madeleine_Udell1, teeni@tauex.tau.ac.il, ~Iddo_Drori1","ChatGPT, reviewing research papers, LLM watermarking, long context windows, fine-tuning, randomized human blind evaluation",datasets and benchmarks,ICLR 2024 Conference Withdrawn Submission,2024,https://openreview.net/pdf?id=gqmELEp3lZ,https://openreview.net/forum?id=gqmELEp3lZ,,,
v0QOVSVPtq,Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering,"Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STAR (**St**iefel-based **A**ctivation Steering for Diverse **R**easoning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STAR collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STAR maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STAR consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.",,,"activation steering, generation diversity, manifold opimization",optimization,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=v0QOVSVPtq,https://openreview.net/forum?id=v0QOVSVPtq,,,
R5sVzzXhW8n,Demystifying How Self-Supervised Features Improve Training from Noisy Labels,"The advancement of self-supervised learning (SSL) motivates researchers to apply SSL on other tasks such as learning with noisy labels. Recent literature indicates that methods built on SSL features can substantially improve the performance of learning with noisy labels. Nonetheless, the deeper reasons why (and how) SSL features benefit the training from noisy labels are less understood. In this paper, we study why and how self-supervised features help networks resist label noise using both theoretical analyses and numerical experiments. Our result shows that, given a quality encoder pre-trained from SSL, a simple linear layer trained by the cross-entropy loss is theoretically robust to symmetric label noise. Further, we provide insights for how knowledge distilled from SSL features can alleviate the over-fitting problem. We hope our work provides a better understanding for learning with noisy labels from the perspective of self-supervised learning and can potentially serve as a guideline for further research.","Hao Cheng, Zhaowei Zhu, Xing Sun, Yang Liu",,"Learning with noisy labels, Self-Supervised Learning",,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=R5sVzzXhW8n,https://openreview.net/forum?id=R5sVzzXhW8n,"~Hao_Cheng5, ~Zhaowei_Zhu1, ~Xing_Sun1, ~Yang_Liu3",1630000000000.0,
8mal7Q96Fc,Mitigating Classifier Dimensional Collapse via Random Masking in Federated Linear Probing,"Integrating a pre-trained model into federated learning (FL) is an emergent direction to facilitate the industrial deployment. Federated linear probing (FLP) is a practical paradigm that gains the communication efficiency from FL and generalization from pre-trained model, while still suffering from dimensional collapse that undermines its effectiveness. Dimensional collapse, originating from data heterogeneity, challenges the embedding space construction in FL, leading to suboptimal convergence and generalization. With a generalized frozen embedding extractor, FLP seems to be robust against dimensional collapse. However, in this paper, we emphasize that the dimensional collapse can also be represented in classifier construction, affecting the performance of the model. We propose FedRM to solve this problem, which randomly masks the dimension of the embedding and the classifier during the training to enforce the classifier to focus fairly on each dimension, guaranteeing diversity during decision generation. The simplicity of the method retains the communication efficiency of the FLP. We conduct empirical experiments to comprehensively evaluate the performance of FedRM. The results show FedRM achieves an overwhelming trade-off between efficiency and utility.",,,"federated learning, data heterogeneity, dimensional collapse, pre-trained model","other topics in machine learning (i.e., none of the above)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=8mal7Q96Fc,https://openreview.net/forum?id=8mal7Q96Fc,,,
RM8JAy8wmP,ChainInfer: A Joint Method for Inferring Missing AI Supply Chain Information,"The modern AI ecosystem forms an intricate AI supply chain, where models, datasets, and software components are deeply interconnected. Incomplete or inconsistent metadata on platforms such as Hugging Face and Kaggle leaves critical gaps in provenance, hindering reproducibility, risk management, and governance. To address this, we formalize AI supply chain inference as a coupled graph learning problem: link prediction to recover missing dependencies and edge classification to determine their semantic types. We propose ChainInfer, a hybrid architecture that integrates graph neural networks for local structural reasoning with graph transformers for global context, trained end-to-end on attributed supply chain graphs. Using a benchmark of 200K models from Hugging Face, ChainInfer outperforms GNN-, Transformer-, and ensemble baselines, achieving 0.93 joint accuracy while remaining efficient. Moreover, ChainInfer generalizes inductively to Kaggle, retaining 0.90 accuracy without retraining. These results demonstrate ChainInfer as a practical framework for scalable, accurate, and transferable AI supply chain provenance inference.",,,"AI supply chain, graph neural network, graph transformer, link prediction, edge classification",learning on graphs and other geometries & topologies,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=RM8JAy8wmP,https://openreview.net/forum?id=RM8JAy8wmP,,,
FJWT0692hw,SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking,"In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented without adversarial training or major architectural changes. We identify the SequenceMatch-χ2 divergence as a more suitable training objective for autoregressive models which are used for generation. We show that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models and arithmetic","Chris Cundy, Stefano Ermon","~Chris_Cundy1, ~Stefano_Ermon1","Sequence Modelling, Imitiation Learning, Language Modelling",generative models,ICLR 2024 poster,2024,https://openreview.net/pdf?id=FJWT0692hw,https://openreview.net/forum?id=FJWT0692hw,,,
r1eVX0EFvH,Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness,"Adversarial examples are malicious inputs crafted to cause a model to misclassify them. In their most common instantiation, ""perturbation-based"" adversarial examples introduce  changes to the input that leave its true label unchanged, yet result in a different model prediction.  Conversely, ""invariance-based"" adversarial examples insert changes to the input that leave the model's prediction unaffected despite the underlying input's label having changed. So far, the relationship between these two notions of adversarial examples has not been studied, we close this gap.

We demonstrate that solely achieving perturbation-based robustness is insufficient for complete adversarial robustness. Worse, we find that classifiers trained to be Lp-norm robust are more vulnerable to invariance-based adversarial examples than their undefended counterparts. We construct theoretical arguments and analytical examples to justify why this is the case. We then illustrate empirically that the consequences of excessive perturbation-robustness can be exploited to craft new attacks. Finally, we show how to attack a provably robust defense --- certified on the MNIST test set to have at least 87% accuracy (with respect to the original test labels) under perturbations of Linfinity-norm below epsilon=0.4 --- and reduce its accuracy (under this threat model with respect to an ensemble of human labelers) to 60% with an automated attack, or just 12% with human-crafted adversarial examples.","Jörn-Henrik Jacobsen, Jens Behrmann, Nicholas Carlini, Florian Tramèr, Nicolas Papernot",,"Invariance, Robustness, Adversarial Examples",,ICLR.cc/2020/Conference,2020,https://openreview.net/pdf?id=r1eVX0EFvH,https://openreview.net/forum?id=r1eVX0EFvH,"j.jacobsen@vectorinstitute.ai, jensb@uni-bremen.de, nicholas@carlini.com, tramer@cs.stanford.edu, nicolas.papernot@utoronto.ca",1570000000000.0,
AmKn0wA652,VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge,"Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. 
To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. 
VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning.
Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks.  
Additionally, reasoning enhancements such as scaling up inference compute (with ''thinking'' modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.",,,"MLLM, benchmark, reasoning",datasets and benchmarks,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=AmKn0wA652,https://openreview.net/forum?id=AmKn0wA652,,,
a3OY2j9kJc-,MaSS: Multi-attribute Selective Suppression,"The recent rapid advances in the development and deployment of machine learning technologies largely depend on the vast richness of data available today, in terms of both the quantity and the rich content contained within. For example, biometric data such as images and voices could reveal people's attributes like age, gender, sentiment, and origin, whereas location/motion data could be used to infer people's activity levels, transportation modes, and life habits. Along with the new services and applications enabled by such technological advances, various governmental policies are put in place to regulate such data usage and protect people's privacy and rights. As a result, data owners often opt for simple data obfuscation (e.g., blur people's faces in images) or withholding data altogether, which leads to severe data quality degradation and greatly limits the data's potential utility.
Aiming for a sophisticated mechanism which gives data owners fine-grained control while retaining the maximal degree of data utility, we propose Multi-attribute Selective Suppression, or MaSS, a general framework for performing precisely targeted data surgery to simultaneously suppress any selected set of attributes while preserving the rest for downstream machine learning tasks. MaSS learns a data modifier through adversarial games between two sets of networks, where one is aimed at suppressing selected attributes, and the other ensures the retention of the rest of the attributes via general contrastive loss as well as explicit classification metrics. We carried out an extensive evaluation of our proposed method using multiple datasets from different domains including facial images, voice audio, and video clips, and obtained highly promising results in MaSS' generalizability and capability of drastically suppressing targeted attributes (e.g., reducing inference on such attributes to random guess) while imposing virtually no impact on the data's usability in other downstream ML tasks.","Chun-Fu Chen, Shaohan Hu, Zhonghao Shi, Prateek Gulati, Bill Moriarty, Marco Pistoia, Vincenzo Piuri, Pierangela Samarati",,"Multi-attribute, GAN, Attribute Suppression",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=a3OY2j9kJc-,https://openreview.net/forum?id=a3OY2j9kJc-,"~Chun-Fu_Chen1, shaohan.hu@jpmchase.com, zhonghas@usc.edu, ~Prateek_Gulati1, william.r.moriarty@jpmchase.com, marco.pistoia@jpmchase.com, vincenzo.piuri@unimi.it, pierangela.samarat@unimi.it",1660000000000.0,
mMLI1u1UvQ,Estimating structural shifts in graph domain adaptation via pairwise likelihood maximization,"Graph domain adaptation (GDA) emerges as an important problem in graph machine learning when the distribution of the source graph data used for training is different from that of the target graph data used for testing. While much of the prior work on GDA has focused on the idea of aligning node representations across source and target domains, recent studies show that such approaches can be suboptimal in the presence of conditional structure shift (CSS), where the distribution of graph edges conditioned on labels changes across domains. In this work, we develop a unified framework to solve CSS and show that existing GDA methods for CSS arise as special cases of our framework. This framework further allows us to develop a new method, Pairwise-Likelihood maximization for graph Structure Alignment (PLSA), which uses rich information from pairwise nodes and edges to improve the estimation of target connection probabilities. We establish conditions under which our method is identifiable and introduce a simple edge reweighting scheme based on importance weights to align the source and target graphs. Theoretically, under the contextual stochastic block model (CSBM), we derive finite-sample guarantees using recent results in matrix concentration inequalities for U-statistics. We complement our theoretical results with empirical studies that demonstrate the effectiveness of our method.",,,"Graph domain adaptation, node classification, graph structure shift, distribution matching, U-statistics","transfer learning, meta learning, and lifelong learning",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=mMLI1u1UvQ,https://openreview.net/forum?id=mMLI1u1UvQ,,,
V6BjBgku7Ro,Planning from Pixels using Inverse Dynamics Models,"Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.","Keiran Paster, Sheila A. McIlraith, Jimmy Ba",,"model based reinforcement learning, deep reinforcement learning, multi-task learning, deep learning, goal-conditioned reinforcement learning",,ICLR.cc/2021/Conference,2021,https://openreview.net/pdf?id=V6BjBgku7Ro,https://openreview.net/forum?id=V6BjBgku7Ro,"~Keiran_Paster1, ~Sheila_A._McIlraith1, ~Jimmy_Ba1",1600000000000.0,
Cox6AaRyan,Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception,"Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire high-resolution images is computationally prohibitive. 
While recent methods leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present a difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the model's internal attention are computationally inefficient and less accurate, requiring either multi-pass prefill stages or reliance on the slow auto-regressive decoding process.
In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around a pipeline that transforms the noisy attention maps from the MLLM's middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train a lightweight Region Proposal Network (RPN) that learns a more precise localization. This RPN is also highly efficient, predicting the RoI in a single forward pass using features from the MLLM's middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass operations.
To validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over a 10\% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning.",,,"MLLM, Self-Distillation, Fine-Grained Perception","foundation or frontier models, including LLMs",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=Cox6AaRyan,https://openreview.net/forum?id=Cox6AaRyan,,,
IhOeYKqnfp,Continual Memory Neurons,"Learning with neural networks by continuously processing a stream of data is very related to the way humans learn from perceptual information. However, when data is not i.i.d., it is largely known that it is very hard to find a good trade-off between plasticity and stability, frequently resulting in catastrophic forgetting issues. In this paper, to our best knowledge, we are the first to follow a significantly novel route, tackling the problem at the lowest level of abstraction. We propose a neuron model, referred to as Continual Memory Neuron (CMN), which does not only compute a response to an input pattern, but also diversifies computations to preserve what was previously learned, while being plastic enough to adapt to new knowledge. The values attached to weights are computed as a function of the neuron input, which acts as a query in a key-value map, with the goal of selecting and blending a set of learnable memory units. We show that this computational scheme is motivated by and strongly related to the ones of popular models that perform computations relying on a set of samples stored in a memory buffer, including Kernel Machines and Transformers. Experiments on class-and-domain incremental streams processed in online and single-pass manner support CMNs' capability to mitigate forgetting, while keeping competitive or better performance with respect to continual learning methods that explicitly store and replay data over time.","Matteo Tiezzi, Simone Marullo, Federico Becattini, Stefano Melacci","~Matteo_Tiezzi1, ~Simone_Marullo1, ~Federico_Becattini2, ~Stefano_Melacci1","Neuron Model, Online Continual Learning, Replay-buffer-free Learning, Self-organized Memories","transfer learning, meta learning, and lifelong learning",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=IhOeYKqnfp,https://openreview.net/forum?id=IhOeYKqnfp,,,
Y4UliyX3LE,Learning Dispersed Embeddings on Hyperspheres,"Learning well-separated features in high-dimensional spaces, such as text or image $\textit{embeddings}$, is crucial for many machine learning applications. Achieving such separation can be effectively accomplished through the $\textit{dispersion}$ of embeddings, where unrelated vectors are pushed apart as much as possible. By constraining features to be on a $\textit{hypersphere}$, we can connect dispersion to well-studied problems in mathematics and physics, where optimal solutions are known for limited low-dimensional cases. However, in representation learning we typically deal with a large number of features in high-dimensional space, which makes leveraging existing theoretical and numerical solutions impossible. Therefore, we rely on gradient-based methods to approximate the optimal dispersion on a hypersphere. In this work, we first give an overview of existing methods from disconnected literature. Next, we propose new reinterpretations of known methods, namely Maximum Mean Discrepancy (MMD) and Lloyd’s relaxation algorithm. Finally, we derive a novel dispersion method that directly exploits properties of the hypersphere. Our experiments show the importance of dispersion in image classification and natural language processing tasks, and how algorithms exhibit different trade-offs in different regimes.","Evgeniia Tokarchuk, Hua Chang Bakker, Vlad Niculae","~Evgeniia_Tokarchuk1, ~Hua_Chang_Bakker1, ~Vlad_Niculae2","embeddings, dispersion, hypersphere, representation learning, separation","unsupervised, self-supervised, semi-supervised, and supervised representation learning",Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=Y4UliyX3LE,https://openreview.net/forum?id=Y4UliyX3LE,,,
pkwq3F7gUp,MILPnet: A Multi-Scale Architecture with Geometric Feature Sequence Representations for Advancing MILP Problems,"We propose MILPnet, a multi-scale hybrid attention framework that models Mixed Integer Linear Programming (MILP) problems as geometric sequences rather than graphs. This approach directly addresses the challenge of Foldable MILP instances, a class of problems that graph-based models, specifically Graph Neural Networks (GNNs), fail to distinguish due to expressiveness limits imposed by the Weisfeiler-Lehman test. By representing MILPs through sequences of constraint and objective features, MILPnet captures both local and global geometric structure using a theoretically grounded multi-scale attention mechanism. We theoretically prove that MILPnet can approximate feasibility, optimal objective value, and optimal solution mappings over a measurable topological space with arbitrarily small error. Empirically, MILPnet outperforms graph-based methods by multiple orders of magnitude in feasibility prediction accuracy and convergence speed on Foldable MILPs, while using significantly fewer parameters. It also generalizes effectively across problem scales and demonstrates strong performance on real-world MILP benchmarks when integrated into an end-to-end solver pipeline.Our code is available with the https://anonymous.4open.science/r/MILPnet-2BD1/",,,MILP;optimal,optimization,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=pkwq3F7gUp,https://openreview.net/forum?id=pkwq3F7gUp,,,
lNIj5FdXsC,Recurrent Distance-Encoding Neural Networks for Graph Representation Learning,"Graph neural networks based on iterative one-hop message-passing have been shown to struggle in harnessing information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but suffer from high computational complexity and have to rely on ad-hoc positional encodings to bake in the graph inductive bias. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates nodes at different distances and uses a parallelizable linear recurrent network over the chain of distances to provide a natural encoding of its neighborhood structure. With no need for positional encoding, we empirically show that the performance of our model is competitive compared with that of state-of-the-art graph transformers on various benchmarks, at a drastically reduced computational complexity. In addition, we show that our model is theoretically more expressive than one-hop message-passing neural networks.","Yuhui Ding, Antonio Orvieto, Bobby He, Thomas Hofmann","~Yuhui_Ding1, ~Antonio_Orvieto3, ~Bobby_He1, ~Thomas_Hofmann1","Recurrent Neural Networks, Graph Neural Networks",learning on graphs and other geometries & topologies,Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=lNIj5FdXsC,https://openreview.net/forum?id=lNIj5FdXsC,,,
jZffxvubJ9,Treatment Rule Optimization Under Counterfactual Temporal Point Processes with Latent States,"In high-stakes areas like healthcare, retrospective counterfactual analysis—such as evaluating what might have happened if treatments were administered earlier, later, or differently—is vital for refining treatment strategies. This paper proposes a counterfactual treatment optimization framework using temporal point processes to model outcome event sequences. By sampling potential outcome events under new treatment decision rules, our approach seeks to optimize treatment strategies in a counterfactual setting. To achieve accurate counterfactual evaluation of new decision rules, we explicitly introduce latent states into the modeling of temporal point processes. Our method first infers the latent states and associated noise, followed by counterfactual sampling of outcome events. This approach rigorously addresses the complexities introduced by latent states, effectively removing biases in the evaluation of treatment strategies. By proving the identifiability of model parameters in the presence of these states, we provide theoretical guarantees that enhance the reliability and robustness of the counterfactual analysis. By incorporating latent states and proving identifiability, our framework not only improves the accuracy and robustness of treatment decision rules but also offers actionable insights for optimizing healthcare interventions. This method holds significant potential for improving treatment strategies, particularly in healthcare scenarios where patient symptoms are complex and high-dimensional.","Xinye Chen, Gegenduolan Zhang, Yinghao Fu, Chao Yang, Shuang Li","~Xinye_Chen2, ~Gegenduolan_Zhang1, ~Yinghao_Fu1, ~Chao_Yang9, ~Shuang_Li3","counterfactual reasoning, temporal point processes, latent confounder, rule learning",interpretability and explainable AI,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=jZffxvubJ9,https://openreview.net/forum?id=jZffxvubJ9,,,
DUtfVs1xxD,Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction,"Safety constitutes a foundational imperative for autonomous driving systems, necessitating maximal incorporation of accessible prior information. This study establishes that temporal perception buffers and cost-efficient high-definition (HD) maps inherently form complementary prior sources for online vectorized HD map construction. We present Uni-PrevPredMap, a unified prior-informed framework systematically integrating previous predictions with corrupted HD maps. Our framework introduces a tri-mode paradigm maintaining operational consistency across non-prior, temporal-prior, and temporal-map-fusion modes. This tri-mode paradigm simultaneously decouples the framework from ideal map assumptions while ensuring robust performance in both map-present and map-absent scenarios. Additionally, we develop a tile-indexed 3D vectorized global map processor enabling efficient 3D prior data refreshment, compact storage, and real-time retrieval. Uni-PrevPredMap achieves state-of-the-art map-absent performance across established online vectorized HD map construction benchmarks. When provided with corrupted HD maps, it exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between temporal predictions and imperfect map data. Code is available in supplementary materials.",,,"online vectorized HD map construction, prior-informed modeling, unified framework","applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=DUtfVs1xxD,https://openreview.net/forum?id=DUtfVs1xxD,,,
yLv6eSBmA-,Adversarial Detector for Decision Tree Ensembles Using Representation Learning,"Research on adversarial evasion attacks focuses mainly on neural network models. Among other reasons, this is because of their popularity in certain fields (e.g., computer vision and NLP) and the models' properties, making it easier to search for adversarial examples with minimal input change. Decision trees and tree ensembles are still very popular due to their high
performance in fields dominated by tabular data and their explainability. In recent years, several works have defined new adversarial attacks targeting decision trees and tree ensembles. As a result, several papers were published focusing on robust versions of tree ensembles. This research aims to create an adversarial detector for attacks on an ensemble of decision trees. While several previous works have demonstrated the generation of more robust tree ensembles, the process of considering evasion attacks during ensemble generation can affect model performance. We demonstrate a method to detect adversarial samples without affecting either the target model structure or its original performance. We showed that by using representation learning based on the structure of the trees, we achieved better detection rates than the state-of-the-art technique and better than using the original representation of the dataset to train an adversarial detector.","Gal Braun, Lior Rokach",,"Machine Learning, Representation Learning, Adversarial Learning, Evasion Attacks, Adversarial Detection, Tree Ensembles, Decision Trees",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=yLv6eSBmA-,https://openreview.net/forum?id=yLv6eSBmA-,"~Gal_Braun1, ~Lior_Rokach2",1660000000000.0,
hqkN6lE1fFQ,Kernel Deformed Exponential Families for Sparse Continuous Attention,"Attention mechanisms take an expectation of a data representation with respect to probability weights. This creates summary statistics that focus on important features. Recently, Martins et al. (2020, 2021) proposed continuous attention mechanisms, focusing on unimodal attention densities from the exponential and deformed exponential families: the latter has sparse support. Farinhas et al. (2021) extended this to use Gaussian mixture attention densities, which are a flexible class with dense support. In this paper, we extend this to two general flexible classes: kernel exponential families and our new sparse counterpart kernel deformed exponential families. Theoretically, we show new existence results for both kernel exponential and deformed exponential families, and that the deformed case has similar approximation capabilities to kernel exponential families. Experiments show that kernel deformed exponential families can attend to non-overlapping intervals of time.","Alexander Moreno, Supriya Nagesh, Zhenke Wu, Walter Dempsey, James Matthew Rehg",,"kernel methods, attention mechanism, theory, exponential families, deformed exponential families",,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=hqkN6lE1fFQ,https://openreview.net/forum?id=hqkN6lE1fFQ,"~Alexander_Moreno1, ~Supriya_Nagesh1, ~Zhenke_Wu1, wdem@umich.edu, ~James_Matthew_Rehg1",1630000000000.0,
H1g79ySYvB,Revisiting Gradient Episodic Memory for Continual Learning,"Gradient Episodic Memory (GEM) is an effective model for continual learning, where each gradient update for the current task is formulated as a quadratic program problem with inequality constraints that alleviate catastrophic forgetting of previous tasks. However, practical use of GEM is impeded by several limitations: (1) the data examples stored in the episodic memory may not be representative of past tasks; (2)  the inequality constraints appear to be rather restrictive for competing or conflicting tasks; (3) the inequality constraints can only avoid catastrophic forgetting but can not assure positive backward transfer. To address these issues, in this paper we aim at improving the original GEM model via three handy techniques without extra computational cost. Experiments on MNIST Permutations and incremental CIFAR100 datasets demonstrate that our techniques enhance the performance of GEM remarkably. On CIFAR100 the average accuracy is improved from 66.48% to 68.76%, along with the backward (knowledge) transfer growing from 1.38% to 4.03%.","Zhiyi Chen, Tong Lin*",,,,ICLR.cc/2020/Conference,2020,https://openreview.net/pdf?id=H1g79ySYvB,https://openreview.net/forum?id=H1g79ySYvB,"chenzhiy16@mails.tsinghua.edu.cn, lintong@pku.edu.cn",1570000000000.0,
Xy1Lf7uR9H,ANOVA-NODE: An identifiable neural network for the functional ANOVA model for better interpretability,"Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. 
The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions so called components, is one of the most popular tools for interpretable AI, and recently, various neural network models have been developed for estimating each component in the functional ANOVA model. 
However, such neural networks are highly unstable when estimating components since the components themselves are not uniquely defined. 
That is, there are multiple functional ANOVA decompositions for a given function. 
In this paper, we propose a novel interpretable model which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably. 
We call our proposed model ANOVA-NODE since it is a modification of Neural Oblivious Decision Ensembles (NODE) for the functional ANOVA model. 
Theoretically, we prove that ANOVA-NODE can approximate a smooth function well.
Additionally, we experimentally show that ANOVA-NODE provides much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural network models do.","Seokhun Park, Insung Kong, yongchan Choi, Chanmoo Park, Yongdai Kim","~Seokhun_Park1, ~Insung_Kong1, ~yongchan_Choi1, ~Chanmoo_Park1, ~Yongdai_Kim1","Interpretability, Explianability, Trustworthy AI, Functional ANOVA model, Generalized additive models",interpretability and explainable AI,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=Xy1Lf7uR9H,https://openreview.net/forum?id=Xy1Lf7uR9H,,,
i0VqD2KaYt,ViT-UWA: Vision Transformer Underwater-Adapter for Dense Predictions Beneath the Water Surface,"Vision Transformer (ViT) and its variants have witnessed a significant success in computer vision. However, they do not perform well in underwater dense prediction tasks due to challenges like complex underwater environments, quality degradation, and light scattering in underwater images. To solve this problem, we propose the  Vision Transformer Underwater-Adapter (ViT-UWA), the first detail-focused and adapted ViT backbone for underwater dense prediction tasks, without requiring task-specific pretraining. In ViT-UWA, we first introduce High-frequency Components Prior (HFCP) to add high-frequency information of underwater images to the plain ViT, which can help recover and capture lost high-frequency information of underwater images. Then, we propose an Detail Aware Module (DAM) to obtain a detail-focused multi-scale convolutional feature pyramid, which can be used in kinds of dense prediction tasks. Through the ViT-CNN Interaction Module (VCIM), we achieve bidirectional feature fusion between ViT and CNN. We evaluate ViT-UWA on multiple underwater dense prediction tasks, including semantic segmentation, instance segmentation, and object detection. Notably, with only ImageNet-22K pretraining, our ViT-UWA-B yields state-of-the-art 46.4 box AP and 44.2 mask AP on USIS10K dataset. We hope ViT-UWA could provide a new backbone for future research on underwater dense prediction tasks.","Qirui LIN, Hua Li, Yuheng Jia, Yutong Li, Shijie Lian, Huazhong Liu, Sam Kwong, Runmin Cong","~Qirui_LIN1, ~Hua_Li8, ~Yuheng_Jia1, ~Yutong_Li4, ~Shijie_Lian1, ~Huazhong_Liu1, ~Sam_Kwong1, ~Runmin_Cong1","Underwater Image Dense Prediction, Adapted ViT Backbone","applications to computer vision, audio, language, and other modalities",Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=i0VqD2KaYt,https://openreview.net/forum?id=i0VqD2KaYt,,,
2YDSK1eiiD,What Can an LLM Flip if It Fails to Flip Coins?,"Large language models (LLMs) can often accurately describe probability distributions using natural language, yet they still struggle to generate faithful samples from them. This mismatch limits their use in tasks requiring reliable stochasticity, such as Monte Carlo methods, agent-based simulations, and randomized decision-making. We investigate this gap between knowledge and sampling in the context of Bernoulli distributions. We introduce Verbalized Rejection Sampling (VRS), a natural-language adaptation of classical rejection sampling that prompts the LLM to reason about and accept or reject proposed samples. Despite relying on the same Bernoulli mechanism internally, VRS substantially reduces sampling bias across models. We provide theoretical analysis showing that, under mild assumptions, VRS improves over direct sampling, with gains attributable to both the algorithm and prompt design. More broadly, our results show how classical probabilistic tools can be verbalized and embedded into LLM workflows to improve reliability, without requiring access to model internals or heavy prompt engineering.",,,"rejection sampling, large language models","probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=2YDSK1eiiD,https://openreview.net/forum?id=2YDSK1eiiD,,,
d159zNCmOq,From Static to Dynamic: Leveraging Implicit Behavioral Models to Facilitate Transition in Offline-to-Online Reinforcement Learning,"Transitioning reinforcement learning (RL) models from offline training environments to dynamic online settings faces critical challenges because of the distributional shift and the model inability in effectively adapting to new, unseen scenarios. This work proposes the \textbf{B}ehavior \textbf{A}daption \textbf{Q}-Learning (BAQ), a novel framework facilitating smoother transitions in offline-to-online RL. BAQ strategically leverages the implicit behavioral model to imitate and adapt behaviors of offline datasets, enabling the model to handle out-of-distribution state-action pairs more effectively during its online deployment. The key to our approach is the integration of a composite loss function that not only mimics the offline data-driven policy but also dynamically adjusts to new experiences encountered online. This dual-focus mechanism enhances the model's adaptability and robustness, reducing Q-value estimation errors and improving the overall learning efficiency. Extensive empirical evaluations demonstrate that BAQ significantly outperforms existing methods, achieving enhanced adaptability and reduced performance degradation in diverse RL settings. Our framework sets a new standard for offline-to-online RL, offering a robust solution for applications requiring reliable transitions from theoretical training to practical, real-world execution.","Lipeng Zu, Xiaonan Zhang, Yukun Yuan, Shayok Chakraborty","~Lipeng_Zu1, ~Xiaonan_Zhang1, ~Yukun_Yuan1, ~Shayok_Chakraborty1","Offline-to-Online Reinforcement Learning, Behavioral Adaptation, Q-value Estimation, Priority Sampling Strategy",reinforcement learning,Submitted to ICLR 2025,2025,https://openreview.net/pdf?id=d159zNCmOq,https://openreview.net/forum?id=d159zNCmOq,,,
dYaeDrazj5,From Images to Connections: Can DQN with GNNs learn the Strategic Game of Hex?,"The gameplay of strategic board games such as chess, Go and Hex is often characterized by combinatorial, relational structures---capturing distinct interactions and non-local patterns---and not just images. Nonetheless, most common self-play reinforcement learning (RL) approaches simply approximate policy and value functions using convolutional neural networks (CNN).
A key feature of CNNs, is their relational inductive biases towards locality and translational invariance. In contrast, graph neural networks (GNN) can encode more complicated and distinct relational structures. Hence, we investigate the crucial question: Can GNNs, with their ability to encode complex connections, replace CNNs in self-play reinforcement learning? To this end, we do a comparison with Hex---an abstract yet strategically rich board game---serving as our experimental platform. Our findings reveal
that GNNs excel at dealing with long range dependency situations in game states and are less prone to overfitting, but also showing a reduced proficiency in discerning local patterns. This suggests a potential paradigm shift, signaling the use of game-specific structures to reshape self-play reinforcement learning.","Yannik Keller, Jannis Blüml, Gopika Sudhakaran, Kristian Kersting","~Yannik_Keller1, ~Jannis_Blüml1, ~Gopika_Sudhakaran1, ~Kristian_Kersting1","Self-play Reinforcement Learning, Graph Neural Networks, Hex, Long Range Dependency Problems, Board games",reinforcement learning,ICLR 2024 Conference Withdrawn Submission,2024,https://openreview.net/pdf?id=dYaeDrazj5,https://openreview.net/forum?id=dYaeDrazj5,,,
HCSgyPUfeDj,Learning and Evaluating Representations for Deep One-Class Classification,"We present a two-stage framework for deep one-class classification. We first learn self-supervised  representations from one-class data, and then build one-class classifiers on learned representations. The framework not only allows to learn better representations, but also permits building one-class classifiers that are faithful to the target task. We argue that classifiers inspired by the statistical perspective in generative or discriminative models are more effective than existing approaches, such as a normality score from a surrogate classifier. We thoroughly evaluate different self-supervised representation learning algorithms under the proposed framework for one-class classification. Moreover, we present a novel distribution-augmented contrastive learning that extends training distributions via data augmentation to obstruct the uniformity of contrastive representations. In experiments, we demonstrate state-of-the-art performance on visual domain one-class classification benchmarks, including novelty and anomaly detection. Finally, we present visual explanations, confirming that the decision-making process of deep one-class classifiers is intuitive to humans. The code is available at https://github.com/google-research/deep_representation_one_class.
","Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister",,"deep one-class classification, self-supervised learning",,ICLR.cc/2021/Conference,2021,https://openreview.net/pdf?id=HCSgyPUfeDj,https://openreview.net/forum?id=HCSgyPUfeDj,"~Kihyuk_Sohn1, ~Chun-Liang_Li1, ~Jinsung_Yoon1, minhojin@google.com, ~Tomas_Pfister1",1600000000000.0,
Hklo5RNtwS,Behavior-Guided Reinforcement Learning,"We introduce a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. We show that by utilizing the dual formulation of the WD, we can learn score functions over trajectories that can be in turn used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, the dual formulation allows us to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. We incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-Guided Evolution Strategies, which we demonstrate can outperform existing methods in a variety of challenging environments. We also provide an open source demo.","Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska, Krzysztof Choromanski, Michael I. Jordan",,"Reinforcement Learning, Optimal Transport, Evolution Strategies",,ICLR.cc/2020/Conference,2020,https://openreview.net/pdf?id=Hklo5RNtwS,https://openreview.net/forum?id=Hklo5RNtwS,"pacchiano@berkeley.edu, jh3764@columbia.edu, yt2541@columbia.edu, achoroma@gmail.com, kchoro@google.com, jordan@cs.berkeley.edu",1570000000000.0,
SwIkknEqmt,Dealing with Frequency Collapse in Time Series Embeddings by Post-Embedding reMapping,"Transformer-based methods have made significant strides in time series forecasting tasks in recent years. However, we observe underfitting in numerous samples, e.g., pattern shifts or excessive deviation in extreme value regions when testing the transform-based model that converges on the training set. Through the proposed spectral analysis of adjacent embedding sequences, we identify a frequency collapse issue in the embedding features generated by the top layer of the transformer backbone. To address this, we propose the Post-Embedding ReMapping (PErM) strategy that improves the frequency-domain representation of embeddings using fixed non-linear functions. Both two kinds of PErM functions that we insert into the model can effectively resolve the frequency collapse issue and lead to significant improvements in prediction performance. Experimental results show that our method outperforms state-of-the-art algorithms across multiple datasets. We will release our code after the review phase.","Haotian Si, Changhua Pei, Haiming Zhang, Dan Pei, Gaogang Xie, Jianhui li","~Haotian_Si1, ~Changhua_Pei2, ~Haiming_Zhang2, ~Dan_Pei1, ~Gaogang_Xie2, ~Jianhui_li4","Time Series Forcasting, deep learning, spectral analysis",learning on time series and dynamical systems,ICLR 2025 Conference Withdrawn Submission,2025,https://openreview.net/pdf?id=SwIkknEqmt,https://openreview.net/forum?id=SwIkknEqmt,,,
HxHrRUHMOD,Accurate Differential Operators for Neural Fields,"Neural fields have become widely used in various fields, from shape representation to neural rendering, and for solving partial differential equations (PDEs). With the advent of hybrid neural field representations like Instant NGP that leverage small MLPs and explicit representations, these models train quickly and can fit large scenes. Yet in many applications like rendering and simulation, hybrid neural fields can cause noticeable and unreasonable artifacts. This is because they do not yield accurate spatial derivatives needed for these downstream applications. In this work, we propose two ways to circumvent these challenges. Our first approach is a post hoc operator that uses local polynomial-fitting to obtain more accurate derivatives from pre-trained hybrid neural fields. Additionally, we also propose a self-supervised fine-tuning approach that refines the neural field to yield accurate derivatives directly while preserving the initial signal. We show the application of our method on rendering, collision simulation, and solving PDEs. We observe that using our approach yields more accurate derivatives, reducing artifacts and leading to more accurate simulations in downstream applications.","Aditya Chetan, Guandao Yang, Zichen Wang, Steve Marschner, Bharath Hariharan","~Aditya_Chetan1, ~Guandao_Yang1, ~Zichen_Wang10, ~Steve_Marschner1, ~Bharath_Hariharan3","Neural Fields, Deep Learning","representation learning for computer vision, audio, language, and other modalities",Submitted to ICLR 2024,2024,https://openreview.net/pdf?id=HxHrRUHMOD,https://openreview.net/forum?id=HxHrRUHMOD,,,
sqVgpiCoKU,IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data,"The potential of three-dimensional molecular generation for structure-based drug discovery is hampered by the scarcity of public protein-ligand complexes, which causes models to overfit and fail to learn generalizable geometric priors. To address this challenge, we employ the PAC-Bayes information bottleneck framework to systematically quantify the information density of three generation paradigms: Scaffold Hopping (SH), Side-Chain Decoration (SC), and De Novo Design (DN). Our analysis reveals that SH possesses the highest information density, which tightens the model's generalization bound and enhances its transferability compared to conventional de novo generation. Motivated by this finding, we propose IBEX, a novel decoupled generation framework. IBEX is trained exclusively on the information-rich SH task to structure its latent representation of chemical space, which is then directly applied to de novo generation in a zero-shot transfer setting. Subsequently, a rapid physical refinement module utilizes the L-BFGS algorithm to optimize each conformer's geometry and binding compatibility by adjusting five short-range interaction terms and six degrees of freedom. Evaluated in a rigorous zero-shot setting on the CBGBench CrossDocked2020-based dataset, IBEX demonstrates substantial improvements over the TargetDiff baseline. It increases the docking success rate from 53% to 64% and improves the average Dock score from -7.41 to -8.09 kcal/mol. Notably, IBEX achieves a superior median Vina energy in 57 out of 100 binding pockets. Furthermore, IBEX enhances drug-likeness by approximately 25% while maintaining state-of-the-art validity and diversity, all corresponding to a demonstrably reduced generalization error. Our results validate that this decoupled approach, which synergizes information-dense pre-training with physical refinement, enables robust zero-shot structure generation and cross-pocket generalization in data-limited regimes.",,,"Molecular Generation, Diffusion Model, Information Bottleneck","applications to physical sciences (physics, chemistry, biology, etc.)",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=sqVgpiCoKU,https://openreview.net/forum?id=sqVgpiCoKU,,,
YmONQIWli--,Gotta Go Fast When Generating Data with Score-Based Models,"Score-based (denoising diffusion) generative models have recently gained a lot of success in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data to noise and generate data by reversing it (thereby going from noise to data). Unfortunately, current score-based models generate data very slowly due to the sheer number of score network evaluations required by numerical SDE solvers. 
   
In this work, we aim to accelerate this process by devising a more efficient SDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which uses a fixed step size. We found that naively replacing it with other SDE solvers fares poorly - they either result in low-quality samples or become slower than EM. To get around this issue, we carefully devise an SDE solver with adaptive step sizes tailored to score-based generative models piece by piece. Our solver requires only two score function evaluations, rarely rejects samples, and leads to high-quality samples. Our approach generates data 2 to 10 times faster than EM while achieving better or equal sample quality. For high-resolution images, our method leads to significantly higher quality samples than all other methods tested. Our SDE solver has the benefit of requiring no step size tuning.","Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, Ioannis Mitliagkas",,"score-based, generative model, denoising diffusion, SDE, diffusion process",,ICLR.cc/2022/Conference,2022,https://openreview.net/pdf?id=YmONQIWli--,https://openreview.net/forum?id=YmONQIWli--,"~Alexia_Jolicoeur-Martineau1, ~Ke_Li1, ~Rémi_Piché-Taillefer1, ~Tal_Kachman1, ~Ioannis_Mitliagkas1",1630000000000.0,
vvD4ilobth,TCL-VS: Temporal Contrastive Learning for Self-Supervised Video Summarization,"The goal of video summarization is to extract the most important parts from the original video. 
Most existing methods are based on supervised learning and they have demonstrated superior performance. 
However, the scarcity of annotated data is a major obstacle in the video summarization task. 
To reduce the impact of the scarcity, some weakly-supervised and unsupervised methods were proposed. 
Although they manifested positive results, existing methods ignore the intrinsic association between video clips. 
To address it, we introduce a new self-supervised learning method called TCL-VS. Our main insight is that 
a excellent summary requires not only maintaining the original video content but also eliminating redundant information.
Inspired by the observation, this work consists of two separate modules that respectively conduct 
temporal consistency and diversity assessment of video clips. Each module predicts a sequence score by clip, 
and then we combine them using a weighted method. Extensive experiments demonstrate that 
our method achieves state-of-the-art performance on two video summarization benchmarks: SumMe and TVSum.","Jiajun Han, Xuran Yang, kun shen Xu, Hui Zhang, Kanle Shi","~Jiajun_Han1, ~Xuran_Yang1, ~kun_shen_Xu1, ~Hui_Zhang1, ~Kanle_Shi1","Video Summarization, Self-Supervised, Contrastive Learning","representation learning for computer vision, audio, language, and other modalities",ICLR 2024 Conference Withdrawn Submission,2024,https://openreview.net/pdf?id=vvD4ilobth,https://openreview.net/forum?id=vvD4ilobth,,,
0yqWGNFEJA,DMIL-Net: A Multi-View Fusion and Region Decoupling Network For Diffusion-Based Generative Image Forgery Localization,"With the increasing application of image generation technology in artistic creation and image editing, its potential for misuse in image forgery has also become increasingly prominent, posing new challenges to verifying image authenticity. In response to this issue, we propose the DMIL-Net. Specifically, we first design a multi-view feature learning strategy combining RGB views, noise views, and high-frequency information to fully capture clues from forgery regions. Secondly, we introduce multi-level contrastive learning to capture long-term dependencies across different modalities, leading to better fusion of multi-view features. Finally, we propose a forgery region decoupling and integration strategy, which iteratively decouples and integrates the body region and detail region to generate complete and detail-accurate localization results. In addition, we construct the DMI dataset, which contains 50,000 generative forgery images created via five prevalent diffusion-based generative image forgery methods, to support model training and testing. Experimental results show that DMIL-Net outperforms five mainstream methods on localization performance, generalization, extensibility, and robustness.",,,Generative Image Forgery Localization; Image Manipulation Detection; Forgery Image Dataset;  Diffusion Model,"applications to computer vision, audio, language, and other modalities",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=0yqWGNFEJA,https://openreview.net/forum?id=0yqWGNFEJA,,,
ijEi63QLsr,Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods,"Prior human-object interaction (HOI) detection methods have integrated early vision-language models (VLMs) such as CLIP, but only as supporting components within their frameworks. In contrast, recent advances in large, generative VLMs suggest that these models may already possess strong ability to understand images involving HOI. This naturally raises an important question: can general-purpose standalone VLMs effectively solve HOI detection, and how do they compare with specialized HOI methods? Answering this requires a benchmark that can accommodate both paradigms. However, existing HOI benchmarks such as HICO-DET were developed before the emergence of modern VLMs, and their evaluation protocols require exact matches to annotated HOI classes. This is poorly aligned with the generative nature of VLMs, which often yield multiple valid interpretations in ambiguous cases. For example, a static image may capture a person mid-motion with a frisbee, which can plausibly be interpreted as either ""throwing"" or ""catching"". When only ""catching"" is annotated, the other, though equally plausible for the image, is marked incorrect when exact matching is used. As a result, correct predictions might be penalized, affecting both VLMs and HOI-specific methods. To avoid penalizing valid predictions, we introduce a new benchmark that reformulates HOI detection as a multiple-answer multiple-choice task, where each question includes only ground-truth positive options and a curated set of negatives that are constructed to reduce ambiguity (e.g., when ""catching"" is annotated, ""throwing"" is not selected as a negative to avoid penalizing valid predictions). The proposed evaluation protocol is the first of its kind for both VLMs and HOI methods, enabling direct comparison and offering new insight into the current state of progress in HOI understanding.",,,Human-Object Interaction; Vision-Language Model,datasets and benchmarks,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=ijEi63QLsr,https://openreview.net/forum?id=ijEi63QLsr,,,
#NAME?,Task-Aware Information Routing from Common Representation Space in Lifelong Learning,"Intelligent systems deployed in the real world suffer from catastrophic forgetting when exposed to a sequence of tasks. Humans, on the other hand, acquire, consolidate, and transfer knowledge between tasks that rarely interfere with the consolidated knowledge.  Accompanied by self-regulated neurogenesis, continual learning in the brain is governed by the rich set of neurophysiological processes that harbor different types of knowledge which are then integrated by the conscious processing. Thus, inspired by Global Workspace Theory of conscious information access in the brain, we propose TAMiL, a continual learning method that entails task-attention modules to capture task-specific information from the common representation space. We employ simple, undercomplete autoencoders to create a communication bottleneck between the common representation space and the global workspace, allowing only the task-relevant information to the global workspace, thereby greatly reducing task interference. Experimental results show that our method outperforms state-of-the-art rehearsal-based and dynamic sparse approaches and bridges the gap between fixed capacity and parameter isolation approaches while being scalable. We also show that our method effectively mitigates catastrophic forgetting while being well-calibrated with reduced task-recency bias.","Prashant Shivaram Bhat, Bahram Zonooz, Elahe Arani",,"Continual learning, Lifelong learning, Representation learning, Global workspace theory, Task-specific attention",,ICLR.cc/2023/Conference,2023,https://openreview.net/pdf?id=-M0TNnyWFT5,https://openreview.net/forum?id=-M0TNnyWFT5,"~Prashant_Shivaram_Bhat1, ~Bahram_Zonooz1, ~Elahe_Arani1",1660000000000.0,
hS9FLRBf7v,Network of Patterns: Time Series Forecasting with Pattern Passing,"Time series contain diverse pattern information, and many studies have leveraged these patterns to enhance representations for more accurate forecasting. A key challenge lies in how to organize multi-scale patterns for effective information aggregation. Previous studies typically partition sequences into multi-scale pattern segments and organize them into chain or tree structures, employing neural networks to aggregate features and improve predictive performance. However, information transmission in chain structures is strictly linear and accumulative, while tree structures can aggregate multiple patterns but remain constrained by hierarchical limitations. Moreover, segments at the same or neighboring scales do not necessarily exhibit strong dependencies.

To overcome these limitations, we propose the Network of Patterns (NoP), which flexibly connects all relevant pattern segments to enable interactions between any nodes. We further introduce a Pattern Passing strategy to efficiently propagate and aggregate pattern information across this network, achieving more comprehensive integration. Experimental results demonstrate that NoP not only effectively encapsulates informative pattern signals but also establishes new state-of-the-art performance on multiple time series forecasting benchmarks, surpassing chain- and tree-based methods.",,,"Time Series Forecasting, Decomposition, Multi-scale",learning on time series and dynamical systems,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=hS9FLRBf7v,https://openreview.net/forum?id=hS9FLRBf7v,,,
rqzZDh8jqGj,Experimental Design for Overparameterized Learning with Application to Single Shot Deep Active Learning,"Abstract The impressive performance exhibited by modern machine learning models hinges on the ability to train such models on a very large amounts of labeled data. However, since access to large volumes of labeled data is often limited or expensive, it is desirable to alleviate this bottleneck by carefully curating the training set. Optimal experimental design is a well-established paradigm for selecting data point to be labeled so to maximally inform the learning process. Unfortunately, classical theory on optimal experimental design focuses on selecting examples in order to learn underparameterized (and thus, non-interpolative) models, while modern machine learning models such as deep neural networks are overparameterized, and oftentimes are trained to be interpolative. As such, classical experimental design methods are not applicable in many modern learning setups. Indeed, the predictive performance of underparameterized models tends to be variance dominated, so classical experimental design focuses on variance reduction, while the predictive performance of overparameterized models can also be, as is shown in this paper, bias dominated or of mixed nature. In this paper we propose a design strategy that is well suited for overparameterized regression and interpolation, and we demonstrate the applicability of our method in the context of deep learning by proposing a new algorithm for single shot deep active learning.","Neta Shoham, Haim Avron",,,,ICLR.cc/2021/Conference,2021,https://openreview.net/pdf?id=rqzZDh8jqGj,https://openreview.net/forum?id=rqzZDh8jqGj,"~Neta_Shoham1, ~Haim_Avron4",1600000000000.0,
ZmhpqpKzAT,IGC-Net for conditional average potential outcome estimation over time,"Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. However, many existing methods for this task fail to properly adjust for time-varying confounding and thus yield biased estimates. There are only a few neural methods with proper adjustments, but these have inherent limitations (e.g., division by propensity scores that are often close to zero), which result in poor performance. As a remedy, we introduce the iterative G-computation network (IGC-Net). Our IGC-Net is a novel, neural end-to-end model which adjusts for time-varying confounding in order to estimate conditional average potential outcomes (CAPOs) over time. Specifically, our IGC-Net is the first neural model to perform fully regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our IGC-Net across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.",,,"causal inference, potential outcomes, treatment effects, healthcare",causal reasoning,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=ZmhpqpKzAT,https://openreview.net/forum?id=ZmhpqpKzAT,,,
cKAUvMePUN,Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses,"Visual prostheses are potential devices to restore vision for blind people, which highly depends on the quality of stimulation patterns of the implanted electrode array. However, existing processing frameworks prioritize the generation of stimulation while disregarding the potential impact of restoration effects and fail to assess the quality of the generated stimulation properly. In this paper, we propose for the first time an end-to-end visual prosthesis framework (StimuSEE) that generates stimulation patterns with proper quality verification using V1 neuron spike patterns as supervision. StimuSEE consists of a retinal network to predict the stimulation pattern, a phosphene model, and a primary vision system network (PVS-net) to simulate the signal processing from the retina to the visual cortex and predict the firing rate of V1 neurons. Experimental results show that the predicted stimulation shares similar patterns to the original scenes, whose different stimulus amplitudes contribute to a similar firing rate with normal cells. Numerically, the predicted firing rate and the recorded response of normal neurons achieve a Pearson correlation coefficient of 0.78.","Chuanqing Wang, Di Wu, Chaoming Fang, Jie Yang, Mohamad Sawan","~Chuanqing_Wang2, ~Di_Wu10, ~Chaoming_Fang1, ~Jie_Yang16, ~Mohamad_Sawan1","Visual prostheses, Spiking recurrent neural network, Dynamic vision sensor, Biological phosphene model",applications to neuroscience & cognitive science,ICLR 2024 poster,2024,https://openreview.net/pdf?id=cKAUvMePUN,https://openreview.net/forum?id=cKAUvMePUN,,,
NnSLujLSfn,Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence,"Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research.",,,"Large Language Model, Cybersecurity, Cyber Threat Intelligence (CTI), Failure Analysis, LLM Robustness","alignment, fairness, safety, privacy, and societal considerations",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=NnSLujLSfn,https://openreview.net/forum?id=NnSLujLSfn,,,
6pmxBlGXyM,​Aligning Stance Dynamics in Foundation Dialogue Models: Towards Context-Aware Safety Control​,"Neural dialogue foundation models exhibit critical safety vulnerabilities by implicitly amplifying toxic discourse through contextual stance alignment. Current mitigation strategies fail to address the nuanced interplay between conversational dynamics and implicit harm. We propose a paradigm shift with three core contributions:
First, we introduce ​context-aware stance classifiers​ leveraging graph-based reasoning and contrastive learning. These models decode complex stance expressions—including sarcastic agreement and indirect bias—that perpetuate harmful echo chambers.
Second, we design ​dynamic safety controllers​ combining attribute-guided decoding with retrieval-augmented counter-speech generation. This hybrid approach steers foundation models toward constructive responses while preserving conversational integrity.
Third, we establish ​multidimensional harm metrics​ quantifying implicit biases across intersectional identities, moving beyond surface-level toxicity. Our framework pioneers joint optimization of stance neutrality, bias mitigation, and fluency preservation.
Validated against adversarial conversational contexts, our method demonstrates significant improvements in ethical alignment for generative dialogue systems. We open-source toolkit facilitates safer deployment of foundation language models.","Rong Fu, GUO XIAOYU, Simon James Fong","~Rong_Fu2, ~GUO_XIAOYU1, ~Simon_James_Fong2","Stance Alignment, Contextual Offensive Language, Controllable Text Generation, Neural Dialogue Safety, Foundation Models, Societal Impact","alignment, fairness, safety, privacy, and societal considerations",ICLR 2026 Conference Withdrawn Submission,2026,https://openreview.net/pdf?id=6pmxBlGXyM,https://openreview.net/forum?id=6pmxBlGXyM,,,
i8PjQT3Uig,Locality Sensitive Sparse Encoding for Learning World Models Online,"Acquiring an accurate world model $\textit{online}$ for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even with very high dimensional nonlinear features. We validate the representation power of our encoding and verify that it allows efficient online learning under data covariate shift. We also show, in the Dyna MBRL setting, that our world models learned online using a $\textit{single pass}$ of trajectory data either surpass or match the performance of deep world models trained with replay and other continual learning methods.","Zichen Liu, Chao Du, Wee Sun Lee, Min Lin","~Zichen_Liu1, ~Chao_Du1, ~Wee_Sun_Lee1, ~Min_Lin1","model-based rl, online learning, incremental learning, catastrophic forgetting",reinforcement learning,ICLR 2024 poster,2024,https://openreview.net/pdf?id=i8PjQT3Uig,https://openreview.net/forum?id=i8PjQT3Uig,,,
7WsivwyHrS,You Only Query Once: An Efficient Label-Only Membership Inference Attack,"As one of the privacy threats to machine learning models, the membership inference attack (MIA) tries to infer whether a given sample is in the original training set of a victim model by analyzing its outputs. Recent studies only use the predicted hard labels to achieve impressive membership inference accuracy. However, such label-only MIA approach requires very high query budgets to evaluate the distance of the target sample from the victim model's decision boundary.  
   We propose YOQO, a novel label-only attack to overcome the above limitation.YOQO aims at identifying a special area (called improvement area) around the target sample and crafting a query sample, whose hard label from the victim model can reliably reflect the target sample's membership. YOQO can successfully reduce the query budget from more than 1,000 times to only ONCE. Experiments demonstrate that YOQO is not only as effective as SOTA attack methods, but also performs comparably or even more robustly against many sophisticated defenses.","YUTONG WU, Han Qiu, Shangwei Guo, Jiwei Li, Tianwei Zhang","~YUTONG_WU3, ~Han_Qiu3, ~Shangwei_Guo1, ~Jiwei_Li1, ~Tianwei_Zhang1","Machine learning, Membership Inference Attack, Computer Vision","societal considerations including fairness, safety, privacy",ICLR 2024 poster,2024,https://openreview.net/pdf?id=7WsivwyHrS,https://openreview.net/forum?id=7WsivwyHrS,,,
8FihPljvWf,Video-As-Prompt: Unified Semantic Control for Video Generation,"Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for this task with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7\% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various applications mark a significant advance toward general-purpose, controllable video generation.",,,"Video Generation, Controllable Video Generation, Video Dataset",generative models,ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=8FihPljvWf,https://openreview.net/forum?id=8FihPljvWf,,,
dfxrltDpBe,Can Large Language Models Really Recognize Your Name?,"Large language models (LLMs) are increasingly being used to protect personal user data. These privacy solutions often assume that LLMs can reliably detect named entities and personally identifiable information (PII). In this paper, we challenge that assumption by revealing how LLMs can regularly overlook broad types of sensitive names even in short text snippets due to ambiguity in the contexts. We construct AMBENCH, a benchmark dataset of seemingly ambiguous yet real entity names designed around the name regularity bias phenomenon and embedded within concise text snippets containing benign prompt injections. Our experiments with state-of-the-art LLMs and specialized PII detection tools show that the recall of AMBENCH names drops by 20--40\% compared to more recognizable names. AMBENCH names are also four times more likely to be ignored in supposedly privacy-preserving LLM-powered text analysis tools adopted in the industry. Our findings showcase blind spots in current LLM-based privacy defenses and call for a systematic investigation into their privacy failure modes.",,,"large language model, privacy, personally identifiable information, named entity detection","alignment, fairness, safety, privacy, and societal considerations",ICLR 2026 Conference Submission,2026,https://openreview.net/pdf?id=dfxrltDpBe,https://openreview.net/forum?id=dfxrltDpBe,,,
